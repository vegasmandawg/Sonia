# SONIA: Final Build Guide
**Document Classification**: Complete Build Reference
**Version**: 2.10.0-dev
**Date**: February 14, 2026
**Format**: 11pt, single-spaced, dense
**Branch**: v2.10-dev (commit bd8b764)
---
## Table of Contents
1. Architecture Overview and Design Philosophy
2. Environment Setup and Prerequisites
3. Shared Infrastructure and Cross-Cutting Concerns
4. Memory Engine: Persistent Knowledge
5. Model Router: Multi-Provider LLM Routing
6. API Gateway: The Central Nervous System
7. OpenClaw: Safe Desktop Execution
8. EVA-OS: Supervisory Control Plane
9. Pipecat: Voice and Modality Gateway
10. Vision and Perception Pipeline
11. MCP Server: External Integration Bridge
12. Configuration Architecture and Wiring
13. Operations Tooling: PowerShell Stack Management
14. Testing Strategy and the 580+ Test Suite
15. Training Pipeline: Fine-Tuning the VLM
16. Avatar System: 3D Embodiment
17. Release Engineering and Promotion Gates
18. Development Chronology: The Six-Day Build
19. Running the Complete Stack
20. What Remains: Production Readiness Gap Analysis
Appendix A: Complete Port Map
Appendix B: Full Technology Stack
Appendix C: Configuration File Index
Appendix D: Test File Inventory
---
## Conventions
Throughout this guide, file paths are relative to the canonical root `S:\` unless otherwise noted. Code blocks show actual implementation patterns from the codebase. PowerShell commands assume execution from `S:\` with the sonia-core conda environment activated. Service names in monospace (e.g., `api-gateway`) refer to the directory under `S:\services\`. The term "turn" refers to a single request-response cycle through the cognitive pipeline. "Correlation ID" refers to the `req_xxx` format identifier that propagates across all service boundaries. All services use the `main:app` pattern with uvicorn and expose health checks at `GET /healthz`.
## Glossary
**SONIA**: Supervised Operational Networked Intelligence Architecture. **EVA-OS**: The supervisory control plane that makes all policy decisions. **OpenClaw**: The desktop automation executor service. **Pipecat**: The voice I/O and streaming service. **Turn**: A single cognitive cycle (memory recall, model chat, tool execution, memory write). **Correlation ID**: A unique identifier (format `req_xxx`) propagated across all service calls for end-to-end tracing. **Ledger**: The append-only event store in Memory Engine's SQLite database. **Circuit Breaker**: A fault-tolerance pattern with three states (CLOSED, OPEN, HALF_OPEN) protecting against cascading failures. **Dead Letter Queue (DLQ)**: A queue capturing failed action executions for later replay. **Approval Token**: A cryptographic, single-use, scope-bound, time-limited token authorizing a guarded action. **HNSW**: Hierarchical Navigable Small World graph for approximate nearest-neighbor vector search. **BM25**: Okapi BM25, a term-frequency/inverse-document-frequency ranking algorithm for keyword search. **GGUF**: A quantized model format used by Ollama for efficient local inference.
---
## Chapter 1: Architecture Overview and Design Philosophy
SONIA -- Supervised Operational Networked Intelligence Architecture -- is a deterministic, voice-first, local-first AI agent platform. The project's founding thesis holds that existing AI assistants suffer from three fundamental weaknesses: they lack persistent memory across sessions, they cannot take reliable action on the user's behalf, and they operate without meaningful safety governance. SONIA was conceived to solve all three simultaneously. The system runs exclusively on Windows 11, uses Python 3.11 with FastAPI/Uvicorn for all services, and is designed for a single trusted operator. The canonical filesystem root is `S:\`, and every operation is hard-scoped to this root as a non-negotiable security boundary enforced at multiple independent layers.
The philosophical underpinning is the principle of "models propose, supervisors decide." In SONIA, no language model output directly causes side effects. Every proposed action passes through a deterministic policy layer (EVA-OS) that classifies risk, requires appropriate approval, and maintains a complete audit trail. This is operational safety for a system that controls a desktop, executes shell commands, reads and writes files, and manages voice conversations.
The initial architectural vision was codified on February 8, 2026, in three foundational documents: `ARCHITECTURE.md` (system design), `ROADMAP.md` (phased delivery from v1.0 through v2.0), and `RUNTIME_CONTRACT.md` (operational guarantees and SLAs). The roadmap originally planned a 14-month trajectory from Q1 2026 through Q2 2027, spanning six major phases (D through I). In practice, the development sprint compressed nearly the entire roadmap into six days of intensive work (February 8-14, 2026), delivering 80 commits, 10 version tags, and 580+ integration tests.
### 1.1 The Five Core Principles
**Deterministic Control.** EVA-OS makes all policy decisions. Language models propose actions; the supervisor decides whether to permit them. This separation ensures that the system's behavior is predictable and auditable regardless of which LLM is generating responses. The classification is based on capability name, not on the LLM's assessment of risk, ensuring that a compromised or hallucinating model cannot escalate its own privileges.
**Composability.** Every service is independently deployable and replaceable. The canonical JSON envelope contract means that any service can be swapped (for example, replacing Pipecat's voice pipeline with a WebRTC implementation) without affecting the rest of the stack. Inter-service communication uses HTTP with correlation IDs, enabling service substitution without protocol changes.
**Observability.** Full causality tracking through correlation IDs (`req_xxx` format) that propagate across every service boundary. Every entry point generates a unique ID, every downstream call carries it, and every log entry includes it. This enables end-to-end request tracing without a distributed tracing backend.
**Safe by Default.** A four-tier risk classification system ensures that read-only operations execute instantly, low-risk modifications have a brief approval gate, medium-risk actions require explicit operator confirmation, and destructive operations demand confirmation plus a verification code. The system defaults to denial -- any action that cannot be classified is blocked.
**Local-First.** All core capabilities run on the local machine. Cloud LLM providers (Anthropic, OpenRouter) are optional enhancements, not requirements. The local Ollama instance provides full conversational capability, while the cloud providers add reasoning depth for complex tasks.
### 1.2 Microservice Topology
SONIA follows a microservices-on-localhost pattern, a deliberate architectural choice that provides service isolation and composability without the overhead of container orchestration. All services are Python/FastAPI applications running on Uvicorn, communicating over plain HTTP on fixed ports (7000-7070). There is no service mesh, no message queue, no container runtime -- all services run as bare processes managed by PowerShell scripts.
The runtime topology consists of 10 services across three tiers. Tier 1 (Core Services, always running): API Gateway on port 7000, Model Router on port 7010, Memory Engine on port 7020, Pipecat on port 7030, OpenClaw on port 7040, and EVA-OS on port 7050. Tier 2 (Vision Services, optional): Vision Capture on port 7060 and Perception on port 7070. Tier 3 (Auxiliary Services, separate lifecycle): Orchestrator on port 8000, MCP Server (stdio/SSE), and Tool Service (legacy, predates OpenClaw).
The startup sequence is dependency-ordered: Memory Engine and Model Router boot first (no dependencies), followed by OpenClaw, then EVA-OS (which probes all upstream services), then Pipecat (depends on Model Router), and finally API Gateway (depends on all). Vision services boot last and are optional. This sequence is enforced by the canonical launcher `start-sonia-stack.ps1`.
### 1.3 Message Contract
The inter-service message contract uses a canonical JSON envelope: `{"message_id": "uuid-v4", "service_from": "api-gateway", "service_to": "model-router", "message_type": "chat_request", "timestamp": "ISO-8601", "body": {...}, "metadata": {"correlation_id": "req_xxx", "trace_id": "uuid-v4", "session_id": "session-xxx"}}`. This envelope provides service-agnostic communication, complete causality tracking, and enables downstream services to operate without knowledge of upstream implementation details.
### 1.4 Trust Boundaries and Risk Model
The trust boundary model has four layers. Layer 1 (Untrusted): Client input, LLM outputs, external APIs -- these are validated and sanitized at the Gateway boundary. Layer 2 (Trusted Policy): API Gateway and EVA-OS form the policy layer where all safety decisions are made. Layer 3 (Trusted Internal): Model Router, Memory Engine, and Pipecat operate within the trusted boundary. Layer 4 (Execution): OpenClaw executes actions only after policy approval, with full audit logging.
The four-tier risk classification: Tier 0 (safe_read) auto-executes with no approval needed, covering file.read, window.list, clipboard.read. Tier 1 (guarded_low) has a 30-second auto-gate, covering app.launch, app.close, browser.open, clipboard.write, window.focus. Tier 2 (guarded_medium) requires explicit operator approval, covering file.write, keyboard.type, keyboard.hotkey, mouse.click. Tier 3 (guarded_high) requires approval plus a verification code, covering shell.run and filesystem.delete.
### 1.5 Data Flow
The primary data flow follows the turn pipeline pattern: (1) Client sends text/voice to API Gateway. (2) Gateway generates correlation ID. (3) Gateway queries Memory Engine for relevant context (token-budgeted). (4) Gateway sends prompt + context to Model Router. (5) Model Router selects provider (Ollama/Anthropic/OpenRouter) and returns response. (6) If response contains tool calls, Gateway classifies risk via tool_policy.py. (7) If approval required, Gateway mints confirmation token and waits. (8) On approval, Gateway executes action through OpenClaw. (9) Gateway writes turn to Memory Engine (raw + summary + tool events). (10) Gateway returns response to client.
---
## Chapter 2: Environment Setup and Prerequisites
### 2.1 Hardware Requirements
SONIA requires Windows 11 (Pro or Enterprise recommended), a minimum of 32GB RAM (64GB recommended for full local inference with all models loaded simultaneously), an NVIDIA GPU with at least 12GB VRAM (24GB+ recommended for running the VLM, embedding model, and ASR model concurrently; an RTX 4090 or equivalent is the recommended target), 200GB+ free disk space (35GB for ML models, 50GB+ for training data and checkpoints, remainder for logs, backups, and development artifacts), and a CUDA 12.8-compatible driver installation.
### 2.2 Software Prerequisites
Install the following software before proceeding. Git for Windows (any recent version). Miniconda or Anaconda for Python environment management. Node.js 20+ (for the UI components and avatar viewer). Ollama (the local LLM inference server, installed to its default location). A text editor or IDE with Python support. PowerShell 5.1 (included with Windows 11). Optional: Blender 3.6+ (for avatar rendering scripts), Unity 2022+ (for avatar animation development).
### 2.3 Python Environment Creation
Create the conda environment at the canonical path. Run: `conda create -p S:\envs\sonia-core python=3.11 -y` followed by `conda activate S:\envs\sonia-core`. Verify with `python --version` (should show Python 3.11.x) and `which python` or `where python` (should show `S:\envs\sonia-core\python.exe`). Note: the Python executable is at `S:\envs\sonia-core\python.exe` directly (conda prefix layout), NOT in a `Scripts/` subdirectory. This is a critical detail that affects all service startup commands.
### 2.4 Installing Dependencies
Install all Python dependencies from the frozen requirements file: `pip install -r S:\requirements-frozen.txt`. This installs 80 packages including: PyTorch 2.10.0+cu128 (with CUDA support), FastAPI 0.116.1, Uvicorn 0.35.0, Pydantic 2.11.7, httpx 0.28.1, websockets 16.0, transformers (for model loading), Unsloth 2026.1.4 (for training), TRL 0.24.0, PEFT 0.18.1, accelerate 1.12.0, bitsandbytes 0.49.1, sentencepiece 0.2.1, tokenizers 0.22.2, and numerous supporting libraries. Verify GPU support: `python -c "import torch; print(torch.cuda.is_available())"` must return True. Verify FastAPI: `python -c "import fastapi; print(fastapi.__version__)"` should show 0.116.1.
### 2.5 Ollama Setup
Install Ollama from https://ollama.ai and verify it is running: `curl http://127.0.0.1:11434/api/tags` should return a JSON response. Pull the required models: `ollama pull qwen2.5:7b` (the lightweight chat model, ~4GB), `ollama pull qwen3-vl:32b-instruct` (the vision-language model, ~20GB). Additional models for the embedding and reranking pipeline (Qwen3-Embedding-8B at 15.1GB and Qwen3-Reranker-8B at 16.4GB) are stored locally as GGUF files in `S:\models\` and loaded directly rather than through Ollama.
### 2.6 Directory Structure
Create the canonical directory structure. The root `S:\` must contain: `config\` (configuration files), `data\` (runtime data including memory.db and sessions), `data\memory\` (SQLite database location), `data\sessions\` (session state files), `data\vector\` (HNSW vector index), `logs\` (all log output), `logs\services\` (per-service stdout/stderr), `logs\gateway\` (structured JSONL logs), `state\` (PID files and lock files), `state\pids\` (process ID tracking), `services\` (all microservice source code), `scripts\` (operational and diagnostic scripts), `tests\` (test suites), `config\schemas\` (JSON schemas for validation), `releases\` (release bundles), `backups\` (state backup snapshots), `training\` (fine-tuning pipeline), `assets\` (avatar and media assets), `ui\` (UI components and web viewer), `models\` (local ML model weights), `docs\` (documentation), and `tools\` (conversion utilities). Most of these directories will be created automatically by the bootstrap process or the services themselves, but the critical paths (`data\memory\`, `logs\services\`, `state\pids\`) must exist before first boot.
### 2.7 Environment Variables
Set the following environment variables. Required: `SONIA_ROOT=S:\` (canonical root path). Optional (for cloud LLM providers): `ANTHROPIC_API_KEY=sk-ant-...` (enables Anthropic Claude models via Model Router), `OPENROUTER_API_KEY=sk-or-...` (enables OpenRouter model access). The system operates fully without cloud API keys using local Ollama models only. The API keys are read by the Model Router's provider implementations at startup.
### 2.8 Verification Checklist
Before proceeding to service implementation, verify: (1) `S:\envs\sonia-core\python.exe --version` shows Python 3.11.x. (2) `pip list | findstr fastapi` shows 0.116.1. (3) `python -c "import torch; print(torch.cuda.is_available())"` returns True. (4) `curl http://127.0.0.1:11434/api/tags` returns valid JSON with at least one model. (5) All required directories exist under `S:\`. (6) `S:\config\sonia-config.json` exists and is valid JSON. (7) PowerShell execution policy allows script execution (`Get-ExecutionPolicy` should not be Restricted).
---
## Chapter 3: Shared Infrastructure and Cross-Cutting Concerns
### 3.1 Version Singleton
Every service imports the canonical version string from `S:\services\shared\version.py`. This file contains a single constant: `SONIA_VERSION = "2.10.0-dev"`. The version is exposed on every `/healthz` endpoint response, ensuring that version drift between services is immediately visible. Services import this via `sys.path.insert(0, r"S:\services\shared")` followed by `from version import SONIA_VERSION`. This sys.path manipulation is necessary because the shared module is not installed as a package -- it lives in the filesystem alongside the services.
### 3.2 Event Envelope System
The shared event system (`S:\services\shared\events.py`) defines the `EventEnvelope` Pydantic model and 20 event types used for inter-service communication. The EventEnvelope contains: `event_id` (UUID), `event_type` (one of 20 typed constants), `source_service` (originating service name), `correlation_id` (req_xxx format), `timestamp` (ISO-8601), `payload` (arbitrary dict), and optional `session_id`. Event types span supervision events (service.healthy, service.degraded, service.unreachable, service.recovered), perception events (perception.trigger, perception.result, perception.error), action events (action.requested, action.approved, action.denied, action.executed, action.failed), session events (session.created, session.closed, session.expired), and system events (system.startup, system.shutdown, system.backup, system.restore). All services that emit events construct EventEnvelope instances and publish them through in-process event handlers. Cross-service event propagation currently uses HTTP calls rather than a message queue -- this is a known architectural limitation documented for future enhancement.
### 3.3 The /healthz Contract
Every service must expose `GET /healthz` returning a JSON response with at minimum `{"ok": true}`. The canonical health response includes: `ok` (boolean), `service` (service name string), `version` (from shared version.py), `uptime_seconds` (float), and optional service-specific fields (e.g., Model Router includes `providers` with per-provider health status, Memory Engine includes `memory_count` and `database_size_bytes`). EVA-OS probes these endpoints on a 30-second interval to maintain the service health registry. The health endpoint path is `/healthz` (not `/health`) -- this distinction caused bugs in early development when configuration files referenced the wrong path.
### 3.4 The main:app Pattern
All services follow an identical startup pattern. Each service has a `main.py` file that: (1) Creates a FastAPI application instance with `app = FastAPI(title="service-name", version=SONIA_VERSION)`. (2) Defines an async lifespan context manager for startup/shutdown logic. (3) Registers routes using `app.include_router()` or direct `@app.get/@app.post` decorators. (4) Exposes the app object as `main:app` for uvicorn. The canonical startup command is: `S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port XXXX --log-level info`. All services bind to 127.0.0.1 (localhost only), never to 0.0.0.0, as a security measure preventing network exposure.
### 3.5 FastAPI Lifespan Pattern
The lifespan pattern was adopted in v2.9 to replace the deprecated `@app.on_event("startup")` pattern. The canonical implementation is: `from contextlib import asynccontextmanager` followed by `@asynccontextmanager` `async def lifespan(app: FastAPI):` with startup logic before `yield` and shutdown logic after. If the lifespan is defined after the `app = FastAPI()` call, it can be assigned via `app.router.lifespan_context = lifespan`. This pattern handles resource initialization (database connections, HTTP client pools, background tasks) and cleanup (closing connections, flushing logs, saving state).
### 3.6 Pydantic v2 Request/Response Models
All API request and response models use Pydantic v2 (version 2.11.7). The canonical pattern uses `BaseModel` with strict type annotations, `Optional` fields with defaults, and field validators where needed. Request models include `model_config = ConfigDict(extra="forbid")` to reject unexpected fields (strict mode). Response models use `model_config = ConfigDict(extra="allow")` to permit additional metadata fields. Schema files are organized per-service: `S:\services\api-gateway\schemas\turn.py`, `schemas\session.py`, `schemas\action.py`, `schemas\vision.py`.
### 3.7 Correlation ID Generation and Propagation
Every entry point (HTTP request handler, WebSocket connection handler) generates a unique correlation ID using the pattern: `correlation_id = f"req_{uuid.uuid4().hex[:12]}"`. This ID is: (1) Included in every log entry as a structured field. (2) Passed to downstream service calls via the `X-Correlation-ID` HTTP header. (3) Returned to the client in response headers. (4) Stored with memory writes for provenance tracking. (5) Included in circuit breaker events and dead letter entries. Stage 7 identified and fixed 5 gaps in correlation ID propagation across `stream.py` and `main.py`, ensuring complete end-to-end coverage.
### 3.8 Structured Logging
All services log in JSONL (JSON Lines) format to `S:\logs\`. The Gateway writes to four structured log files: `turns.jsonl` (complete turn records with latency measurements), `sessions.jsonl` (session lifecycle events), `actions.jsonl` (action execution audit trail with before/after state), and `errors.jsonl` (error events with full correlation context). The `jsonl_logger.py` module in the API Gateway provides the `JSONLLogger` class that handles file rotation, buffered writes, and atomic append operations. Service stdout/stderr is captured to `S:\logs\services\{service-name}.out.log` and `{service-name}.err.log` by the PowerShell launcher scripts.
---
## Chapter 4: Memory Engine -- Persistent Knowledge
### 4.1 Service Overview
The Memory Engine (port 7020) provides SONIA with persistent, searchable memory that survives across sessions, restarts, and updates. It is the foundation for SONIA's ability to recall past interactions, maintain context, and build long-term knowledge. The service is implemented in `S:\services\memory-engine\main.py` (29KB, the second-largest main.py in the codebase) and contains 18 Python source files totaling approximately 120KB of code. The Memory Engine has no upstream dependencies -- it boots first in the startup sequence alongside the Model Router.
### 4.2 Database Layer
The storage backend is SQLite in WAL (Write-Ahead Logging) mode, with the database at `S:\data\memory\memory.db`. WAL mode enables concurrent reads during writes, which is critical for search performance during active conversation -- the Gateway can query for context while simultaneously writing completed turns. The database layer is implemented in `S:\services\memory-engine\db.py`, which provides the `MemoryDatabase` class with connection pooling, ACID transaction guarantees, and schema versioning. The database schema has evolved through 6 SQL migrations located in `S:\services\memory-engine\db\migrations\`: `001_ledger.sql` creates the core append-only event ledger table, `002_workspace.sql` adds the knowledge workspace for structured document storage, `003_snapshots.sql` adds context snapshot storage for point-in-time recall, `004_indexes.sql` adds performance indexes for common query patterns, `005_fts.sql` enables SQLite's FTS5 (Full-Text Search) extension for efficient keyword matching, and `006_provenance.sql` adds the provenance audit log for data lineage tracking. The migration runner (`run_migrations.py`) applies migrations in order, tracking which have been applied in a `_migrations` table. On first boot with a fresh database, all 6 migrations run automatically.
### 4.3 The Append-Only Ledger
The core data model is an append-only event ledger. Each memory entry is an immutable event with: `id` (UUID primary key), `content` (the text content of the memory), `content_type` (one of: fact, preference, summary, observation, tool_event, confirmation_event, raw), `source` (which service or user created it), `importance` (float 0.0-1.0, used in retrieval ranking), `created_at` (ISO-8601 timestamp), `metadata` (JSON blob with arbitrary key-value pairs), and `correlation_id` (linking back to the originating request). The ledger is never modified -- updates create new entries with back-references, and deletes are soft (a `deleted_at` timestamp). This append-only design provides complete audit history and simplifies backup/restore operations.
### 4.4 CRUD Operations
The Memory Engine exposes four primary endpoints. `POST /v1/store` accepts a `StoreRequest` with `content`, `content_type`, `source`, `importance`, and optional `metadata`, creates a UUID, writes to the ledger, updates the BM25 index, and returns the entry ID. `POST /v1/search` (aliased as `/v1/recall`) accepts a `RecallRequest` with `query`, optional `content_types` filter, optional `max_tokens` budget, and `limit`, then executes the hybrid search pipeline and returns ranked results. `GET /v1/entries/{id}` retrieves a specific memory entry by UUID. `DELETE /v1/entries/{id}` performs a soft delete by setting `deleted_at`. The search endpoint is the most complex, implementing the full hybrid retrieval pipeline described below.
### 4.5 Hybrid Search Architecture
The `HybridSearchLayer` class in `S:\services\memory-engine\hybrid_search.py` (6.3KB) combines multiple retrieval strategies for optimal recall. The hybrid approach was chosen over pure vector search because SONIA's memory corpus is relatively small (single-user, single-machine) and keyword-exact queries are common in operational contexts. Pure semantic search would miss queries that depend on exact terminology (for example, searching for a specific filename or error code).
The retrieval pipeline operates in stages: (1) The query arrives at `/v1/search`. (2) BM25 scoring runs first using the FTS5-accelerated inverted index (`S:\services\memory-engine\core\bm25.py`, 6.0KB). BM25 implements the Okapi BM25 ranking algorithm, scoring documents based on term frequency, inverse document frequency, and document length normalization. FTS5 handles tokenization, stemming, stop-word elimination, and Unicode normalization. (3) If BM25 yields fewer than 3 results, SQL LIKE fallback executes as a substring match. A critical lesson learned: LIKE queries must be literal substrings of stored content -- they are not semantic, not fuzzy, and not regex. (4) Results are ranked by a composite score: `relevance * 0.5 + recency * 0.3 + importance * 0.2`, where relevance is the search-method score (BM25 or match confidence), recency is a time-decay factor favoring recent memories, and importance is the per-entry weight set during memory writes. (5) Token budget enforcement truncates results to fit within the configured limit (default 2,000 tokens, approximately 4 characters per token). (6) Type filters are applied if the caller specified content_type preferences (for example, preferring summaries over raw turns). (7) Provenance metadata is attached to each returned result.
### 4.6 Vector Search with HNSW
The `S:\services\memory-engine\vector\hnsw_index.py` (12.3KB) implements an HNSW (Hierarchical Navigable Small World) graph for approximate nearest-neighbor search on dense vector embeddings. The HNSW index enables semantic search -- finding memories that are conceptually related even if they do not share keywords. Embeddings are generated by the Qwen3-Embedding-8B model (15.1GB, GGUF format) via a local inference endpoint. The HNSW index stores these high-dimensional vectors and supports cosine similarity queries. The hybrid search combines BM25 keyword scores with vector similarity scores using configurable weights (default: BM25 0.4, vector 0.6). A critical current limitation: the HNSW index exists only in memory and must be rebuilt on every Memory Engine restart. For the current corpus size this is acceptable, but persistence to disk is planned for v2.11.
### 4.7 Sentence-Level Chunking
The `S:\services\memory-engine\core\chunker.py` (7.1KB), introduced in v2.10, implements sentence-level text chunking for document ingestion. Previous versions used fixed-size character chunks which frequently split sentences mid-thought, producing incoherent memory entries. The new chunker preserves sentence boundaries by detecting period-space, question mark, and exclamation mark patterns, then grouping sentences into chunks that respect a configurable maximum token count. This produces more coherent memory entries that yield better retrieval results.
### 4.8 Memory Decay
The `S:\services\memory-engine\core\decay.py` (9.3KB) implements memory decay strategies that reduce the relevance score of older memories over time. Three decay functions are supported: linear decay (relevance decreases linearly with age), exponential decay (relevance drops rapidly then levels off), and step decay (relevance drops at specific age thresholds). Decay parameters are configurable per content type -- for example, facts decay slowly (they remain relevant for months), while raw conversation turns decay quickly (they become less relevant within days). This prevents ancient context from dominating retrieval results and ensures that recent information is prioritized.
### 4.9 Provenance Tracking
The `S:\services\memory-engine\core\provenance.py` (4.6KB) maintains an audit log of every memory operation. Each provenance record tracks: which service wrote the memory, when it was written, what source document or conversation it came from, what transformations were applied (chunking, summarization, extraction), and the correlation ID linking it to the originating request. The provenance table (`audit_log`) uses a foreign key to the event ledger, creating a complete data lineage chain. This enables compliance reporting ("where did this information come from?") and debugging ("why was this memory returned for this query?").
### 4.10 Building and Verifying
To build and test the Memory Engine independently: (1) Ensure `S:\data\memory\` directory exists. (2) Start the service: `cd S:\services\memory-engine && S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port 7020`. (3) Verify health: `curl http://127.0.0.1:7020/healthz` should return `{"ok": true, "service": "memory-engine", ...}`. (4) Store a test memory: `curl -X POST http://127.0.0.1:7020/v1/store -H "Content-Type: application/json" -d "{\"content\": \"Test memory entry\", \"content_type\": \"fact\", \"source\": \"manual\"}"`. (5) Search for it: `curl -X POST http://127.0.0.1:7020/v1/search -H "Content-Type: application/json" -d "{\"query\": \"test memory\"}"`. (6) Run tests: `pytest S:\tests\integration\test_memory_quality_policy.py -v`.
---
## Chapter 5: Model Router -- Multi-Provider LLM Routing
### 5.1 Service Overview
The Model Router (port 7010) is SONIA's intelligence distribution layer, responsible for selecting the optimal LLM provider for each request based on task type, provider health, and cost constraints. The service is implemented in `S:\services\model-router\main.py` with supporting modules in `S:\services\model-router\app\` and `S:\services\model-router\providers.py`. It has no upstream service dependencies and boots in parallel with the Memory Engine.
### 5.2 Provider Architecture
The Router supports three fully implemented providers, all using raw `httpx` HTTP clients rather than vendor SDKs. This decision was deliberate: it eliminates SDK version coupling, reduces dependency surface area, and provides full control over retry logic, timeout management, and request shaping. Each provider implements a common interface with `chat()`, `health_check()`, and `get_models()` methods.
The **OllamaProvider** communicates with the local Ollama instance at `http://127.0.0.1:11434` using Ollama's native HTTP API. It is the primary (default) provider, offering the lowest latency since inference runs on the local GPU. Supported models include `sonia-vlm:32b` (the fine-tuned VLM), `qwen2.5:7b` (lightweight chat), and any other model pulled into Ollama.
The **AnthropicProvider** communicates with Anthropic's Messages API at `https://api.anthropic.com/v1/messages` using httpx with the `x-api-key` header and `anthropic-version: 2023-06-01` header. It extracts system messages from the conversation and passes them via the `system` parameter (Anthropic's API separates system from user messages). Supported models: claude-opus-4-6, claude-sonnet-4-6, claude-haiku-4-5. The provider requires the `ANTHROPIC_API_KEY` environment variable.
The **OpenRouterProvider** communicates with OpenRouter's OpenAI-compatible API at `https://openrouter.ai/api/v1/chat/completions`. It adds `HTTP-Referer` and `X-Title` headers as required by OpenRouter's terms. This provider gives access to a wide range of models (GPT-4, Llama, Mixtral, etc.) through a single API key. Requires the `OPENROUTER_API_KEY` environment variable.
### 5.3 Routing Engine and Policies
The `routing_engine.py` (6.7KB) in `S:\services\model-router\app\` implements three routing policies. `local_only` restricts all inference to local Ollama models with no cloud calls permitted, suitable for air-gapped or privacy-sensitive deployments. `cloud_allowed` (the default) prefers local models but falls back to cloud providers when local is unavailable, overloaded, or when the task type requires deeper reasoning capabilities. `provider_pinned` forces routing to a specific provider regardless of health or cost, used primarily for testing and debugging.
The routing engine selects providers based on task type. The `TaskType` enum defines: `text` (standard chat), `vision` (requires VLM capability), `embeddings` (vector generation), and `reranker` (result reranking). Each task type has a configured fallback chain: `chat_low_latency` cascades sonia-vlm then qwen2.5 (local only, fast response); `reasoning_deep` cascades claude-opus then claude-sonnet then sonia-vlm (cloud-first for complex reasoning); `vision_analysis` cascades sonia-vlm then qwen3-vl then claude-sonnet (vision-capable models); `tool_execution` cascades sonia-vlm then claude-sonnet then qwen2.5 (reliable tool calling).
### 5.4 Health Registry
The `health_registry.py` (7.2KB) tracks provider health with probe-based monitoring. Each provider has a health score that degrades on failures and recovers on successful probes. The health score is a rolling window (configurable, default 5 minutes) of success/failure ratios. When a provider's health score drops below the threshold (default 0.5), it enters quarantine -- the router excludes it from the fallback chain. Recovery requires three consecutive successful health probes. Health probes run on a background asyncio task at a configurable interval (default 60 seconds). The quarantine mechanism prevents the router from repeatedly attempting to use a provider that is experiencing an outage.
### 5.5 Budget Guard
The `budget_guard.py` (4.6KB) enforces token budget ceilings per request. The guard estimates the total token consumption (input + expected output) using the `_estimate_context_tokens()` function, which counts message tokens by character length with a 4-character-per-token heuristic. If the estimated consumption exceeds the model's context window or a configured cost threshold, the request is rejected or rerouted to a cheaper provider. This prevents runaway costs from cloud LLM calls and protects against accidentally sending enormous context windows.
### 5.6 Generation Profiles
The `profiles.py` (11.8KB) defines six deterministic generation profiles that configure model behavior for different tasks: `chat_default` (temperature 0.7, max_tokens 2048, standard conversation), `precise_reasoning` (temperature 0.2, longer context, for analysis and planning), `creative_generation` (temperature 0.9, for creative writing and brainstorming), `code_generation` (temperature 0.1, optimized for code with stop sequences), `tool_execution` (temperature 0.0, structured output mode for reliable tool calls), and `vision_analysis` (configured for multimodal input processing with image tokens). Profiles are selected by the API Gateway based on the turn context and can be overridden per-request.
### 5.7 Route Audit Logging
The `route_audit.py` (3.8KB) maintains a JSONL audit log of all routing decisions at `S:\logs\services\model-router\routes.jsonl`. Each entry records: the correlation ID, task type, selected provider, selected model, fallback chain evaluated, health scores at decision time, token budget, and latency. This audit trail enables post-hoc analysis of routing behavior and debugging of provider selection issues.
### 5.8 Building and Verifying
To build and test the Model Router independently: (1) Ensure Ollama is running with at least one model. (2) Start: `cd S:\services\model-router && S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port 7010`. (3) Health check: `curl http://127.0.0.1:7010/healthz` should return provider health status. (4) Test chat: `curl -X POST http://127.0.0.1:7010/v1/chat -H "Content-Type: application/json" -d "{\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}"`. (5) Test model selection: `curl -X POST http://127.0.0.1:7010/v1/select -H "Content-Type: application/json" -d "{\"task_type\": \"text\"}"`. (6) Run tests: `pytest S:\tests\model_router\ -v`.
---
## Chapter 6: API Gateway -- The Central Nervous System
### 6.1 Service Overview
The API Gateway (port 7000) is SONIA's largest and most complex service, comprising 37 Python source files totaling over 350KB of code. It serves as the stable front door for all client interactions, orchestrating the entire turn pipeline, managing sessions, handling WebSocket streaming, enforcing tool safety policies, and coordinating the action pipeline. The Gateway depends on all other core services (Model Router, Memory Engine, OpenClaw) and therefore boots last in the startup sequence.
### 6.2 Route Structure
The Gateway exposes five route groups. `POST /v1/turn` is the core cognitive loop: memory recall, model chat, tool execution, memory write. This is the synchronous entry point for text-based interactions and represents the heart of SONIA's intelligence. `WS /v1/stream/{session_id}` provides real-time WebSocket streaming for voice, text, and vision events, handling audio frames, vision frames, and text messages with full session context. `POST/GET/DELETE /v1/sessions` manages session lifecycle with in-memory storage, 30-minute TTL, and a maximum of 100 concurrent sessions. `POST /v1/actions` and `POST /v1/actions/{id}/approve|deny` provide the action pipeline entry point with approval flow for guarded operations. `POST /v1/chat` offers a simple chat endpoint for quick interactions without the full turn pipeline overhead. Additional diagnostic routes include `GET /v1/diagnostics/snapshot` (full system state dump), `GET /v1/breakers/metrics` (circuit breaker event history), `POST/GET /v1/backups` (state backup management), and `GET /v1/audit/actions` (action audit trail).
### 6.3 Client Modules
Three client modules in `S:\services\api-gateway\clients\` handle outbound HTTP calls to downstream services. The `router_client.py` communicates with Model Router on port 7010, supporting task-type routing (chat, vision, reasoning) and passing correlation IDs via the `X-Correlation-ID` header. It uses httpx.AsyncClient with configurable timeouts and retries. The `memory_client.py` communicates with Memory Engine on port 7020, handling search queries with token budget enforcement and memory writes with type classification. The `openclaw_client.py` communicates with OpenClaw on port 7040, managing action execution requests and collecting structured execution results.
### 6.4 Turn Pipeline Implementation
The turn pipeline (`S:\services\api-gateway\routes\turn.py`) implements the four-step cognitive loop. Step 1 (Memory Recall): When a turn request arrives, the Gateway queries Memory Engine for relevant context. The `memory_recall_context.py` (10.0KB) enforces a token budget (default 2,000 tokens) to prevent context window overflow. Retrieval returns memories ranked by composite score with type filters preferring summaries over raw turns. Step 2 (Model Chat): Recalled memories are injected into the model prompt as system context. The Gateway calls Model Router's `/v1/chat` endpoint, which selects the optimal provider based on routing policy and task type. The `model_call_context.py` (8.6KB) wraps this call with cancellation support -- the operator can abort a long-running LLM inference without leaving orphaned requests. Step 3 (Tool Execution): If the model response contains tool calls, the Gateway classifies each through `tool_policy.py`, obtains approval if required, and executes through the action pipeline. The circuit breaker protects against adapter failures, and the dead letter queue captures failed actions. Step 4 (Memory Write): After the turn completes, the Gateway writes conversation context to Memory Engine according to `memory_policy.py`. Five write types are used: raw (unprocessed turns), summary (LLM-generated summaries), vision_observation (scene descriptions), tool_event (tool invocation records), and confirmation_event (approval/denial decisions). The write policy never raises exceptions -- a failing Memory Engine degrades recall quality but does not block conversation.
### 6.5 Turn Quality Controls
The `turn_quality.py` (2.1KB) applies response normalization to every model output: stripping C0 control characters, enforcing maximum output length (4,000 characters default, configurable via `max_output_chars`), rejecting empty responses with a fallback cascade (retry with different profile, then return a graceful error), and adding quality annotations to the response metadata. Annotations include: `generation_profile_used` (which profile generated this response), `fallback_used` (whether fallback was triggered), `tool_calls_attempted` and `tool_calls_executed` (tool execution accounting), and `completion_reason` (normal, max_tokens, error, fallback).
### 6.6 Session Management
The `session_manager.py` (5.7KB) manages in-memory sessions with TTL-based expiration. Sessions are created via `POST /v1/sessions` with optional configuration (preferred model, voice profile, vision settings). Each session tracks: session ID (UUID), creation and last-activity timestamps, session configuration, turn history for context continuity, and pending confirmation tokens (limited to 10 per session to prevent queue flooding). Sessions expire after 30 minutes of inactivity. The maximum concurrent session count is 100, enforced at creation time with HTTP 429 responses when exceeded. Sessions are NOT persisted -- a Gateway restart clears all active sessions. This is intentional for the current single-operator model but represents a known limitation for production deployment.
### 6.7 WebSocket Streaming
The `routes/stream.py` (25.3KB) implements WebSocket streaming for real-time interactions. The streaming protocol supports three event families: `input.*` events (client-to-server: text messages, audio frames, vision frames), `output.*` events (server-to-client: text responses, audio data, status updates), and `control.*` events (bidirectional: vision enable/disable, mode changes, session configuration updates). Vision ingestion is handled through `control.vision.enable` and `control.vision.disable` events, with frame data arriving as `input.vision.frame` events. Vision frames are validated (max 1MB, max 10fps rate limit, max 3 frames per turn) and invalid or oversized frames are rejected non-fatally. Every WebSocket message carries a correlation ID for end-to-end tracing.
### 6.8 Tool Safety Gate
The `tool_policy.py` (10.8KB) classifies every tool invocation into one of four risk tiers: `safe_read` (auto-execute immediately), `guarded_low` (30-second auto-approval gate), `guarded_medium` (explicit operator approval required), and `guarded_high` (approval plus a 6-digit verification code). The classification is deterministic and based solely on capability name -- the system never consults the LLM for risk assessment. The `GatewayConfirmationManager` mints confirmation tokens with 120-second TTL, scope-bound hashes (HMAC-SHA256 of tool name + arguments), and single-use enforcement. Approve and deny operations are idempotent -- replaying an approval or denial has no additional effect. Per-session pending confirmations are limited to 10 to prevent queue flooding attacks.
### 6.9 Action Pipeline
The `action_pipeline.py` (37.2KB) is the single largest module in the entire codebase, implementing the complete desktop action execution lifecycle. The pipeline has 10 stages: (1) Validation -- action request parsed against capability registry. (2) Policy Check -- risk tier classification. (3) Approval Gate -- if required, token minted and returned. (4) Token Validation -- scope hash, expiry, single-use verified. (5) Adapter Selection -- choose executor (ctypes native, PowerShell subprocess, or dry-run). (6) Breaker Check -- verify circuit breaker state for selected adapter. (7) Execution -- run action through selected executor. (8) Result Capture -- collect execution result, timing, state changes. (9) Audit Log -- write complete action record. (10) Dead Letter -- on failure, capture to DLQ with retry classification. Every action supports `dry_run=true` which executes steps 1-4 without performing the actual action, enabling "what would happen" previews.
### 6.10 Circuit Breaker
The `circuit_breaker.py` (10.6KB) implements per-adapter circuit breakers with three states. CLOSED: normal operation, failures counted against a rolling window. OPEN: all requests rejected immediately with a `CIRCUIT_OPEN` error, cooldown timer running (default 30 seconds). HALF_OPEN: a single probe request is allowed; success transitions to CLOSED, failure returns to OPEN. Configuration: 3 consecutive failures trigger OPEN, 30-second cooldown, 1 probe in HALF_OPEN state, metrics tracking bounded to 200 events. Each desktop adapter (ctypes, subprocess, dry-run) has its own breaker instance, so a failure in PowerShell subprocess execution does not block native ctypes operations. The `GET /v1/breakers/metrics` endpoint exposes time-series event data for monitoring.
### 6.11 Dead Letter Queue
The `dead_letter.py` (6.2KB) captures failed action executions for later analysis and replay. Each dead letter contains: the original action request, failure reason and stack trace, timestamp and correlation ID, and retry eligibility classification from the retry taxonomy. Dead letters can be replayed via `POST /v1/dead-letters/{id}/replay`, with an optional `?dry_run=true` parameter to validate the replay without side effects. The DLQ provides a safety net ensuring that transient failures do not permanently lose action requests.
### 6.12 Retry Taxonomy
The `retry_taxonomy.py` (4.5KB) classifies failures into 8 categories with specific retry policies. CONNECTION_BOOTSTRAP (retryable, exponential backoff -- service not yet started). TIMEOUT (retryable, linear backoff -- slow model inference). CIRCUIT_OPEN (not retryable, wait for cooldown). POLICY_DENIED (not retryable -- action blocked by policy). VALIDATION_FAILED (not retryable -- invalid action arguments). EXECUTION_ERROR (maybe retryable, exponential backoff -- runtime error in executor). BACKPRESSURE (retryable, exponential backoff -- too many concurrent actions). UNKNOWN (not retryable -- unclassified failures). This taxonomy ensures that only genuinely transient failures are retried, preventing infinite retry loops on permanent errors.
### 6.13 Memory Policy
The `memory_policy.py` in the Gateway defines bounded context retrieval. The default token budget is 2,000 tokens. Type filters prefer summaries over raw turns (summaries are more information-dense). The policy enforces a maximum of 20 retrieved memories per query. Retrieval results include provenance metadata. The write policy defines five memory types (raw, summary, vision_observation, tool_event, confirmation_event) and ensures that memory writes never raise exceptions -- failures are logged and silently ignored to prevent memory engine outages from blocking conversation.
### 6.14 State Backup
The `state_backup.py` (9.0KB) provides automated state backup and restore with SHA-256 integrity verification. Each backup snapshot (stored in `S:\backups\state\{timestamp}\`) contains: `actions.json` (executed action history), `breakers.json` (per-adapter circuit breaker state), `dead_letters.json` (DLQ contents), and `manifest.json` (SHA-256 checksums for all files). The backup API: `POST /v1/backups` creates a snapshot, `GET /v1/backups` lists available snapshots, `GET /v1/backups/verify` recomputes SHA-256 hashes and validates against the manifest, and `POST /v1/restore/dlq` restores the dead letter queue from a backup.
### 6.15 Additional Gateway Modules
The `operator_session.py` (11.9KB) implements a state machine for operator interactions, tracking the lifecycle from initial connection through active conversation to graceful disconnect. The `perception_action_gate.py` (13.0KB) ensures that perception-triggered actions from the vision pipeline cannot bypass the normal approval workflow -- a fail-closed design where any error results in no action rather than uncontrolled action. The `vision_ingest.py` handles WebSocket vision frame validation and rate limiting. The `health_supervisor.py` (8.6KB) monitors downstream service health from the Gateway's perspective with periodic probes.
### 6.16 Building and Verifying
The API Gateway requires all downstream services running. Start them first (Memory Engine on 7020, Model Router on 7010, OpenClaw on 7040), then: `cd S:\services\api-gateway && S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port 7000`. Verify: `curl http://127.0.0.1:7000/healthz`. Test a turn: `curl -X POST http://127.0.0.1:7000/v1/turn -H "Content-Type: application/json" -d "{\"user_id\": \"test\", \"conversation_id\": \"test-conv\", \"input_text\": \"Hello Sonia\"}"`. Run core tests: `pytest S:\tests\integration\test_turn_pipeline.py -v`.
---
## Chapter 7: OpenClaw -- Safe Desktop Execution
### 7.1 Service Overview
OpenClaw (port 7040) is SONIA's desktop automation service, providing deterministic, policy-governed action execution across 13 desktop capabilities. The service consists of 18 Python source files and implements the execution layer that translates high-level action requests (such as "write this file" or "click at coordinates x,y") into actual operating system operations. OpenClaw enforces the root contract (`S:\` boundary) at the executor level, providing a second layer of defense beyond the Gateway's policy checks.
### 7.2 Capability Registry
The capability registry (implemented in `S:\services\openclaw\registry.py`) defines 13 supported capabilities organized by domain. File operations: `file.read` (safe_read tier, reads file contents within `S:\`), `file.write` (guarded_medium, writes file contents). Shell operations: `shell.run` (guarded_high, executes arbitrary shell commands within root). Application operations: `app.launch` (guarded_low, launches named applications), `app.close` (guarded_low, closes running applications). Clipboard operations: `clipboard.read` (safe_read, reads clipboard text), `clipboard.write` (guarded_low, writes to clipboard). Keyboard operations: `keyboard.type` (guarded_medium, types text strings), `keyboard.hotkey` (guarded_medium, sends keyboard shortcuts). Mouse operations: `mouse.click` (guarded_medium, clicks at screen coordinates). Window operations: `window.list` (safe_read, lists open windows with titles and handles), `window.focus` (guarded_low, brings a window to foreground). Browser operations: `browser.open` (guarded_low, opens a URL in the default browser). Each capability is registered with its risk tier, executor type, argument schema, and description.
### 7.3 Executor Architecture
Three executor implementations provide different execution strategies. The **ctypes (native) executor** (`S:\services\openclaw\executors\desktop_exec.py`, 16.9KB) uses Python's ctypes library to call Win32 API functions directly. This provides sub-200ms execution for desktop operations like mouse clicks (via `SetCursorPos` and `mouse_event`), keyboard input (via `SendInput`), and window management (via `FindWindow`, `SetForegroundWindow`). The direct API access eliminates the overhead of launching subprocesses. The **subprocess (PowerShell) executor** uses PowerShell subprocesses for operations that require system-level access or complex scripting, such as launching applications via `Start-Process` or complex file operations. The SLO for this executor is p95 < 2000ms, reflecting the overhead of process creation and PowerShell initialization. The **dry-run executor** validates action requests without executing them. It runs through all validation, policy checking, and argument verification stages but stops before actual execution. This is used for testing, policy validation, and the "what would happen" preview mode that lets operators see what an action would do before approving it.
### 7.4 Root Contract Enforcement
All filesystem operations enforce the canonical root boundary at `S:\`. The enforcement is implemented at three independent layers: (1) EVA-OS policy layer rejects paths outside the root during policy evaluation. (2) OpenClaw's file executor (`file_exec.py`) validates and canonicalizes all paths before execution, checking for path traversal attempts (e.g., `S:\..\..\Windows\System32\`). (3) The API Gateway's action pipeline performs pre-checks on all file-related actions. A path traversal attack would need to bypass all three layers simultaneously to succeed. The file executor uses `os.path.realpath()` to resolve symlinks and `..` components, then verifies the resolved path starts with `S:\`.
### 7.5 Policy Engine and Confirmations
The `app/policy_engine.py` (17.3KB) evaluates every action request against configured policy before execution. The engine: (1) Classifies the action into a risk tier based on capability name. (2) Checks whether the tier requires approval. (3) If approval required, generates a confirmation token with 120-second TTL. (4) Returns the token to the caller for operator presentation. (5) On approval, validates the token (single-use, scope-bound via HMAC-SHA256, not expired). (6) On execution, logs the complete action with before/after state. The `app/confirmations.py` (16.4KB) manages the confirmation queue. Tokens are single-use (cannot be replayed), scope-bound (the HMAC hash includes the action name and arguments, preventing token reuse for different actions), time-limited (120-second TTL), and per-session (limited to 10 pending confirmations).
### 7.6 Audit Trail
Every action execution is logged to the audit trail with: action ID (UUID), capability name, arguments, risk tier, approval status (auto-approved, manually approved, or denied), executor used (ctypes, subprocess, dry-run), execution result (success/failure), timing measurements (validation_ms, policy_ms, execution_ms, total_ms), before/after state snapshots where applicable, correlation ID, and session ID. The audit trail is accessible via `GET /v1/audit/actions` (paginated) and `GET /v1/audit/actions/{id}` (single entry).
### 7.7 Building and Verifying
Start OpenClaw: `cd S:\services\openclaw && S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port 7040`. Health check: `curl http://127.0.0.1:7040/healthz`. Test a safe_read action: `curl -X POST http://127.0.0.1:7040/execute -H "Content-Type: application/json" -d "{\"tool_name\": \"file.read\", \"arguments\": {\"path\": \"S:\\\\config\\\\sonia-config.json\"}}"`. Run tests: `pytest S:\tests\safety\ -v`.
---
## Chapter 8: EVA-OS -- Supervisory Control Plane
### 8.1 Service Overview
EVA-OS (port 7050) is SONIA's supervisory control plane -- the deterministic brain that makes policy decisions, monitors service health, and enforces operational boundaries. The name EVA-OS reflects its role as the "executive virtual assistant operating system." The service was initially implemented as a skeleton with hardcoded health data in early development stages, then evolved to a real supervisor with live health probing, state machine transitions, and event emission in v2.9 (System Closure).
### 8.2 Service Supervisor
The `service_supervisor.py` (10.8KB, residing in `S:\services\eva-os\` or imported from the shared path) implements a five-state machine for each monitored service. States: UNKNOWN (initial, not yet probed), STARTING (registered but awaiting first successful probe), HEALTHY (responding normally), DEGRADED (responding but with elevated latency exceeding 2-second threshold), and UNHEALTHY (three consecutive failures or timeouts). State transitions: UNKNOWN to STARTING on registration, STARTING to HEALTHY on first successful probe, HEALTHY to DEGRADED when probe succeeds but latency exceeds threshold, DEGRADED to UNHEALTHY on three consecutive failures, UNHEALTHY to HEALTHY on three consecutive successful probes (recovery). The supervisor performs `/healthz` probes against all registered services on a configurable interval (default 30 seconds in production, 15 seconds in the current implementation). Each probe measures response time and validates the JSON response payload.
### 8.3 Event Emission
State transitions emit typed EventEnvelope events through the shared event system. Event types: `supervision.service.healthy` (service transitioned to healthy), `supervision.service.degraded` (latency threshold exceeded), `supervision.service.unreachable` (three consecutive failures), `supervision.service.recovered` (returned to healthy from unhealthy). Events include the service name, previous state, new state, probe latency, and correlation ID. Other services can subscribe to these events to react to health changes -- for example, the Model Router could exclude an unhealthy Ollama provider from its fallback chain.
### 8.4 Dependency Graph
EVA-OS maintains a dependency graph defining which services depend on which others. API Gateway depends on Model Router, Memory Engine, and OpenClaw. Pipecat depends on Model Router and API Gateway. Perception depends on Vision Capture and Model Router. When a service transitions to UNHEALTHY, EVA-OS checks the dependency graph and marks dependent services as DEGRADED, providing early warning of systemic issues. The dependency graph is exposed via `GET /v1/supervision/dependency-graph` for diagnostic inspection.
### 8.5 Policy Enforcement
EVA-OS enforces the root contract (`S:\` filesystem boundary), the risk-tiered approval workflow, and operational policies defined in `S:\config\policies\default.yaml`. All policy decisions are logged with full context (who requested, what was evaluated, what decision was made, and why) for audit compliance. The `SafeOrchestrator` component (imported from the OpenClaw path) handles the policy gating workflow including approval token generation and verification.
### 8.6 Endpoints
EVA-OS exposes: `GET /healthz` (self-health with supervisor state), `GET /status` (aggregated status of all monitored services), `GET /v1/supervision/dependency-graph` (typed service dependencies), `POST /v1/supervision/maintenance-mode` (toggle maintenance mode which suspends automated policy enforcement), `POST /gate-tool-call` (policy evaluation endpoint for tool call gating). The `/status` endpoint returns a map of service names to their current state, last probe time, probe latency, and failure count.
### 8.7 Building and Verifying
EVA-OS should start after the services it monitors. Start: `cd S:\services\eva-os && S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port 7050`. Verify: `curl http://127.0.0.1:7050/healthz`. Check supervision status: `curl http://127.0.0.1:7050/status` (should show health state for all registered services). Run tests: `pytest S:\tests\integration\test_v29_supervision_drills.py -v`.
---
## Chapter 9: Pipecat -- Voice and Modality Gateway
### 9.1 Service Overview
Pipecat (port 7030) implements SONIA's voice-first interaction model, providing real-time voice activity detection, automatic speech recognition (ASR), text-to-speech (TTS) synthesis, and intelligent turn-taking. The service consists of 18 Python source files organized into `routes/` (WebSocket handlers), `app/` (core voice pipeline modules), and `pipeline/` (audio processing components). Pipecat depends on the Model Router (for LLM inference) and integrates with the API Gateway for turn pipeline routing.
### 9.2 Voice Pipeline Architecture
The voice pipeline processes audio in a linear chain: Audio In receives raw PCM or mulaw audio chunks from the client WebSocket. Voice Activity Detection (VAD, `pipeline/vad.py`, 7.5KB) determines when the user is speaking using energy-based and model-based detection with configurable hangover (300ms default -- how long silence must persist before declaring end-of-speech). Automatic Speech Recognition (ASR, `pipeline/asr.py`, 7.7KB) integrates with the locally-deployed faster-whisper-large-v3 model (2.9GB CTranslate2 format), supporting streaming partial transcripts so the UI can display what the user is saying in real-time before the turn is complete. Turn Detection (`app/turn_taking.py`, 5.7KB) determines when the user has finished speaking, considering silence duration (configurable threshold), prosodic cues (intonation patterns), and semantic completeness. The recognized text is routed through the Model Router for LLM inference. Text-to-Speech (TTS, `pipeline/tts.py`, 8.0KB) generates speech from model responses with streaming output so audio playback begins before the entire response is generated. Audio Out sends synthesized audio back through the WebSocket.
### 9.3 Interruption Handling
The `app/interruptions.py` (5.7KB) handles barge-in scenarios where the user begins speaking while SONIA is still responding. The system cancels TTS output within 100ms and returns to listening mode. An interrupt debounce of 150ms prevents false triggers from background noise. The cancel-aware design (`app/` modules with `_cancel_aware` suffix) ensures that interrupted inference requests are cleanly aborted without leaving orphaned model calls.
### 9.4 Session Management
Pipecat maintains its own session layer (`app/session_manager.py`, 11.4KB) tracking voice session state, audio buffer management, and per-session configuration. Voice sessions have a maximum concurrency of 10 (configured in `sonia-config.json`). The `app/watchdog.py` (6.0KB) monitors session health and terminates stale sessions -- if no audio activity occurs for a configurable timeout (default 5 seconds), the watchdog closes the session and frees resources.
### 9.5 Voice Turn Router
The `app/voice_turn_router.py` routes transcribed text through the Gateway's turn pipeline via the `ApiGatewayClient`. This integration ensures that voice turns go through the same memory recall, model chat, tool execution, and memory write steps as text turns, maintaining behavioral consistency regardless of input modality. The voice turn router also handles per-turn telemetry logging (`app/telemetry.py`) writing JSONL records with ASR latency, model latency, TTS latency, and end-to-end turn time.
### 9.6 Audio Configuration
Audio parameters are configured in `sonia-config.json` under the voice section: sample rate 16000Hz, encoding PCM 16-bit or mulaw, samples per chunk 160-480, VAD enabled with 300ms hangover, turn finalization silence 1000ms, barge-in enabled with 100ms cancel latency and 150ms interrupt debounce, maximum concurrent voice sessions 10, target end-to-end latency <200ms.
### 9.7 Building and Verifying
Start Pipecat (requires Model Router running): `cd S:\services\pipecat && S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port 7030`. Verify: `curl http://127.0.0.1:7030/healthz`. Voice testing requires a WebSocket client sending audio frames. Run unit tests: `pytest S:\tests\pipecat\ -v`. Run integration tests: `pytest S:\tests\integration\test_stream_text_fallback.py -v`.
---
## Chapter 10: Vision and Perception Pipeline
### 10.1 Vision Capture Service (Port 7060)
The Vision Capture service manages camera input with a privacy-first design. Implementation in `S:\services\vision-capture\main.py`. The core component is the Ring Buffer: a 300-frame circular buffer stores recent frames, with old frames overwritten to prevent unbounded memory growth. Two capture modes are supported: Ambient mode (320x240 resolution at 1fps for passive awareness) and Active mode (640x480 at 10fps for detailed analysis). The Privacy Hard Gate is the most critical design element: vision is disabled by default (`default_mode: "off"` in configuration). It must be explicitly enabled per session via `control.vision.enable` WebSocket events. There is no ambient capture without explicit operator consent. The service enforces a maximum frame size of 1MB and rate limits to 10fps. The Zero-Frame Guarantee ensures that when vision is disabled, zero frames are captured, processed, or stored anywhere in the system. This guarantee is verified by integration tests. Endpoints: `GET /healthz`, `POST /capture` (request frame capture), `GET /frames/latest` (retrieve latest frame), `POST /privacy/enable|disable` (toggle privacy gate).
### 10.2 Perception Service (Port 7070)
The Perception service runs VLM (Vision-Language Model) inference on captured frames. Implementation in `S:\services\perception\main.py` with the pipeline runner in `S:\services\perception\pipeline_runner.py`. The service is event-driven, responding to four trigger types: `wake_word` (user explicitly requests visual analysis), `motion` (significant motion detected in frames), `user_command` (command referencing visual context), and `scheduled` (periodic awareness updates). The pipeline runner orchestrates: frame retrieval from Vision Capture, VLM inference via Model Router (using the `sonia-vlm:32b` model with `task_type=vision`), and structured SceneAnalysis output describing what is visible, what has changed, and what actions might be relevant. GPU budget enforcement limits inference to 2000ms maximum with a 10-second hard timeout.
### 10.3 Perception-Action Safety
The Perception-Action Gate (`S:\services\api-gateway\perception_action_gate.py`, 13.0KB) ensures that perception-triggered actions are bypass-proof. Even if the VLM suggests an action based on visual analysis, the suggestion must pass through the standard risk classification, approval, and audit pipeline. The gate is fail-closed: any error in the perception pipeline results in a safe default (empty analysis, no recommended actions) rather than an uncontrolled action. Confirmation is required on ALL perception-suggested actions, regardless of their risk tier.
### 10.4 Building and Verifying
Vision services are optional and boot last. Start Vision Capture: `cd S:\services\vision-capture && S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port 7060`. Start Perception: `cd S:\services\perception && S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port 7070`. Verify: `curl http://127.0.0.1:7060/healthz` and `curl http://127.0.0.1:7070/healthz`. Run tests: `pytest S:\tests\integration\test_stream_vision_ingest.py -v`.
---
## Chapter 11: MCP Server -- External Integration Bridge
### 11.1 Service Overview
The MCP (Model Context Protocol) Server exposes SONIA as an MCP-compatible tool provider, enabling Claude Desktop and other MCP clients to interact with SONIA's capabilities. Implementation in `S:\services\mcp-server\server.py` using the `mcp.server.fastmcp.FastMCP` library. This service was introduced in v2.10 and represents the newest component of the architecture.
### 11.2 Transport and Protocol
The server supports two transport modes: stdio (default, for direct process integration) and SSE (Server-Sent Events, accessible via `--sse --port 8080`, for network-accessible integration). In stdio mode, the JSON-RPC protocol runs over stdin/stdout, which means all application logging MUST go to stderr (stdout is the protocol channel). The server reads `S:\config\sonia-config.json` for Gateway endpoint configuration (default `http://127.0.0.1:7000`).
### 11.3 Exposed Capabilities
The server exposes three categories of tools to MCP clients. `sonia_chat` provides direct access to the turn pipeline, sending user messages through the full cognitive loop (memory recall, model chat, tool execution, memory write) and returning the response. The OpenClaw tool catalog exposes filesystem operations (`filesystem.read_file`, `filesystem.write_file`, `filesystem.list_directory`), process operations (`process.list`, `process.kill`), shell operations (`shell.execute`), and HTTP operations (`http.get`, `http.post`). All tool calls go through the standard safety gate -- an MCP client cannot bypass SONIA's risk classification and approval workflow. `sonia_memory_search` provides direct access to the Memory Engine's hybrid search endpoint.
### 11.4 Resources
The server exposes two MCP resources: `memory://stats` returns memory engine statistics (entry count, database size, index status) and `health://services` returns the aggregated health status of all SONIA services (equivalent to EVA-OS's `/status` endpoint).
### 11.5 Building and Verifying
The MCP server requires the Gateway running. For stdio mode (testing): `cd S:\services\mcp-server && S:\envs\sonia-core\python.exe server.py`. For SSE mode: `S:\envs\sonia-core\python.exe server.py --sse --port 8080`. Test with the MCP inspector or Claude Desktop by configuring the server in `~/.claude/mcp_servers.json`. Run tests: `pytest S:\services\mcp-server\test_server.py -v`.
---
## Chapter 12: Configuration Architecture and Wiring
### 12.1 The Canonical Configuration File
`S:\config\sonia-config.json` (227 lines) is the single source of truth for all SONIA configuration. Every service reads this file at startup to discover its own settings and the locations of services it depends on. The file is structured into seven major sections.
The **services** section defines each microservice with: `name` (human-readable), `port` (fixed assignment), `host` (always `127.0.0.1`), `health_endpoint` (`/healthz`), `log_file` (path to stdout log), and `pid_file` (path to PID tracking file). All 8 core services plus the Orchestrator are defined here. The **eva_os** section configures the supervisor: `root_contract` (the `S:\` boundary), `approval_timeout_seconds` (120), `memory_discipline` settings, `health_check_interval_seconds` (30), and policy tier definitions. The **pipecat** section configures voice: `sample_rate` (16000), `encoding` ("pcm_s16le"), `vad_enabled` (true), `vad_hangover_ms` (300), `turn_silence_ms` (1000), `barge_in_enabled` (true), `max_concurrent_sessions` (10), and latency targets. The **memory_engine** section configures storage: `ledger_path` (`S:\data\memory\memory.db`), `knowledge_workspace` path, `durable_types` (which content types survive across sessions), `max_context_tokens` (2000), and decay parameters. The **model_router** section configures LLM routing: `default_model`, `fallback_model`, routing `policy` (default `cloud_allowed`), `profiles` (the six generation profiles), `fallback_matrix` (per-task-type fallback chains), and provider-specific health thresholds. The **openclaw** section configures desktop execution: `tool_catalog_path`, `root_contract`, `deny_escaping_root` (true), `tier_enforcement` settings, and per-capability configuration. The **vision** section configures the visual pipeline: `ring_buffer_size` (300), `default_fps` (1), `active_fps` (10), `ambient_resolution` and `active_resolution`, privacy defaults, and frame size limits.
### 12.2 Supplementary YAML Configuration
Several YAML files provide domain-specific configuration. `S:\config\app.yaml` contains application-level settings including service host/port definitions and general runtime parameters. Note: this file has known stale references (`/health` instead of `/healthz`, `S:\configs` instead of `S:\config`) that should be corrected. `S:\config\models\model-routing.yaml` defines the model routing rules, provider configurations, and fallback chains in YAML format (parallel to the JSON configuration). `S:\config\services\services.yaml` provides an alternative service definition format. `S:\config\voice\voice-profile.yaml` defines voice synthesis profiles (pitch, speed, emphasis patterns) for different conversation contexts. `S:\config\policies\default.yaml` defines the EVA-OS policy rules including tier thresholds, approval requirements, and blocked capability lists. `S:\configs\ports.yaml` provides a flat port-to-service mapping (legacy path, duplicated in the canonical JSON). `S:\configs\logging.yaml` configures log formatters, handlers, and log levels per service. `S:\configs\default.yaml` provides default values for unspecified configuration keys.
### 12.3 Configuration Conflicts and Technical Debt
Three known configuration conflicts exist and should be resolved. First, `app.yaml` references `/health` endpoints while the canonical contract (and all service implementations) use `/healthz`. Second, `app.yaml` references `S:\configs` (the pre-cleanup path) while the canonical path is `S:\config`. Third, `sonia-config.json` references `S:\shared\schemas` (a pre-cleanup path) while schemas now reside at `S:\config\schemas`. Additionally, the dual JSON/YAML configuration format is itself technical debt -- services must choose which file to read, and conflicting values between JSON and YAML sources can cause subtle bugs. The recommended resolution is to converge on `sonia-config.json` as the single authoritative source and treat YAML files as supplementary overrides.
### 12.4 Dependency Locking
`S:\config\dependency-lock.json` provides SHA-256 hashes for all frozen dependencies, enabling integrity verification. The root-level `S:\requirements-frozen.txt` lists 80 packages with exact version pins. Key packages: PyTorch 2.10.0+cu128, FastAPI 0.116.1, Uvicorn 0.35.0, Pydantic 2.11.7, httpx 0.28.1, websockets 16.0, transformers (latest), Unsloth 2026.1.4, and their transitive dependencies. The promotion gate scripts verify dependency integrity by recomputing SHA-256 hashes and comparing against the lock file.
### 12.5 Inter-Service Wiring
All inter-service communication uses plain HTTP on localhost. The Gateway (7000) calls Model Router (7010) via `http://127.0.0.1:7010/v1/chat` and `/v1/select`. The Gateway calls Memory Engine (7020) via `http://127.0.0.1:7020/v1/store` and `/v1/search`. The Gateway calls OpenClaw (7040) via `http://127.0.0.1:7040/execute`. EVA-OS (7050) probes all services via their `/healthz` endpoints. Pipecat (7030) calls Model Router directly for voice inference and the Gateway for turn pipeline routing. Perception (7070) calls Vision Capture (7060) for frame retrieval and Model Router for VLM inference. No service discovery is used -- all URLs are hardcoded based on port assignments from the configuration file.
### 12.6 Secrets Management
API keys for cloud providers are passed via environment variables: `ANTHROPIC_API_KEY` for Anthropic's Claude models and `OPENROUTER_API_KEY` for OpenRouter's model marketplace. These are read by the Model Router's provider implementations at startup. No secrets are stored in configuration files, source code, or version control. For production deployment, a secrets manager (HashiCorp Vault, AWS Secrets Manager, or similar) would replace environment variables.
---
## Chapter 13: Operations Tooling -- PowerShell Stack Management
### 13.1 Shared Library
The foundation of SONIA's operational tooling is `S:\scripts\lib\sonia-stack.ps1`, a PowerShell module that is dot-sourced (`. S:\scripts\lib\sonia-stack.ps1`) by all runner scripts. The library provides: `Get-SoniaRoot` (resolves the canonical root directory), `Get-SoniaConfig` (loads and parses sonia-config.json), `Ensure-Dir` (idempotent directory creation), `Test-PortListen` (checks if a port is in use via netstat), `Test-SoniaServiceHealth` (HTTP health probe with timeout), `Start-SoniaService` (starts a single service with PID tracking, log redirection, and health verification), `Stop-SoniaService` (stops a service by PID with graceful shutdown then force kill), `Get-SoniaServiceStatus` (returns running/stopped status for a service), and `Write-SoniaLog` (structured log output with timestamps). All functions use the canonical configuration file for service definitions, ensuring consistency between the configuration and operational tooling.
### 13.2 Canonical Launcher
The `S:\start-sonia-stack.ps1` script starts the entire SONIA stack in dependency order. The startup sequence: (1) Pre-flight checks -- verify Python environment (`S:\envs\sonia-core\python.exe` exists), verify Ollama is running (probe port 11434), verify at least one model is loaded, verify all required ports are free, check GPU VRAM availability. (2) Start Memory Engine (port 7020) and Model Router (port 7010) in parallel (they have no dependencies on each other). (3) Wait for both to report healthy via `/healthz`. (4) Start OpenClaw (port 7040). (5) Start EVA-OS (port 7050) -- it immediately begins probing all upstream services. (6) Start Pipecat (port 7030) -- depends on Model Router for inference. (7) Start API Gateway (port 7000) -- depends on all services above. (8) Optional: start Vision Capture (7060) and Perception (7070). (9) Post-startup health check -- verify all services healthy. The launcher accepts parameters: `-Reload` (restart already-running services), `-SkipHealthCheck` (skip post-startup verification), `-SkipPreflight` (skip pre-flight checks), `-LaunchUI` (open the companion UI after startup).
### 13.3 Shutdown Script
The `S:\stop-sonia-stack.ps1` script stops all services in reverse dependency order: API Gateway first, then Pipecat, EVA-OS, OpenClaw, Model Router, Memory Engine, and finally Vision services. Each service is stopped by reading its PID from `S:\state\pids\{service-name}.pid`, sending a SIGTERM-equivalent (via `Stop-Process`), waiting up to 5 seconds for graceful shutdown, then force-killing if still running. PID files are cleaned up after shutdown.
### 13.4 Per-Service Runners
Individual service runner scripts in `S:\scripts\ops\` enable starting/stopping single services: `run-api-gateway.ps1`, `run-model-router.ps1`, `run-memory-engine.ps1`, `run-pipecat.ps1`, `run-openclaw.ps1`, `run-eva-os.ps1`. Each script dot-sources the shared library, sets the working directory to the service's source directory, and invokes uvicorn with the correct port and log configuration. The `kill-by-port.ps1` utility provides emergency shutdown by killing any process listening on a specified port.
### 13.5 Diagnostic Scripts
`S:\scripts\diagnostics\doctor.ps1` performs a comprehensive system health check: verifying Python environment, checking all service ports, probing all `/healthz` endpoints, validating configuration file syntax, checking disk space, verifying GPU availability, and reporting any issues found. `S:\scripts\diagnostics\smoke-api.ps1` runs a quick API smoke test: creates a session, sends a turn, verifies the response, and cleans up. `S:\scripts\diagnostics\health-smoke.ps1` probes all service health endpoints and reports status.
### 13.6 Incident Bundle Export
The `S:\scripts\export-incident-bundle.ps1` exports a complete investigation package for incident analysis. The bundle includes: service logs for a configurable time window (`-WindowMinutes` parameter, default 15), health check results for all services, circuit breaker state and event history, dead letter queue contents, system diagnostics snapshot (from `GET /v1/diagnostics/snapshot`), git revision information, and configuration snapshots. The bundle is written to a timestamped directory under `S:\reports\` as a self-contained investigation package.
### 13.7 Key PowerShell Lessons
Several PowerShell-specific lessons were learned during development that are critical for anyone maintaining or extending the operational tooling. (1) `$args` and `$pid` are read-only automatic variables in PowerShell -- never use these as local variable names. (2) PowerShell 5.1 does not support the `??` null-coalescing operator -- use `if ($x) { $x } else { "default" }` instead. (3) Unicode em-dash (--) and arrow characters (to) in `.ps1` string literals cause parse errors in PowerShell 5 -- use ASCII `--` and `->` instead. (4) `$ErrorActionPreference = "Stop"` causes PowerShell to terminate on stderr output from Python, including deprecation warnings from the `websockets` library -- use the `-W ignore` flag when invoking Python. (5) PowerShell 5 string interpolation parses `$var/$var` as a drive path -- use `${var}/${var}` syntax instead. (6) `ConvertFrom-Json` returns PSCustomObject, not a hashtable -- property access works in `$()` expressions but requires care when piping to other commands.
---
## Chapter 14: Testing Strategy and the 580+ Test Suite
### 14.1 Testing Philosophy
SONIA employs a testing philosophy that emphasizes integration tests over unit tests, reflecting the microservices architecture where most bugs manifest at service boundaries rather than within individual modules. The reasoning: a unit test verifying that the circuit breaker transitions from CLOSED to OPEN after 3 failures tells you the breaker works in isolation. An integration test verifying that a turn pipeline survives a Model Router outage by falling back correctly tells you the system works. SONIA prioritizes the latter.
### 14.2 Test Configuration
Test configuration lives in `S:\pytest.ini` with: `asyncio_mode = auto` (all async tests run automatically without explicit markers), `importlib` import mode (for reliable module resolution), and `testpaths = [tests, services]` (test discovery scans both the central test directory and in-service test files). Test markers include: `legacy_v26_v28` (tests for legacy v2.6-2.8 modules), `legacy_voice_turn_router` (legacy voice routing tests), `legacy_manifest_schema` (legacy schema validation), and `infra_flaky` (tests with known timing sensitivity). The `conftest.py` in `S:\tests\integration\` provides shared fixtures including HTTP client factories, mock service stubs, and test data generators.
### 14.3 Test Distribution
The test suite spans 57 files with approximately 580+ tests distributed across: `S:\tests\integration\` (47 files covering all development stages), `S:\tests\model_router\` (4 files: budget_guard, health_quarantine, profile_selection, fallback_matrix), `S:\tests\pipecat\` (3 files: interrupt handling, turn state machine, watchdog), `S:\tests\safety\` (2 files: policy_engine, confirmation_flow), and in-service contract tests (`services/*/test_contract.py`, 5 files with ~75 tests verifying each service's API contract compliance).
### 14.4 Test Evolution by Development Stage
Tests grew in lockstep with features. Stage 2 (turn pipeline): 8 tests in `test_turn_pipeline.py` verifying the four-step cognitive loop. Stage 3 (voice sessions): 25 tests across `test_session_lifecycle.py` (5), `test_stream_text_fallback.py` (4), `test_tool_confirmation_gate.py` (9), and `test_stage2_compat.py` (7 regression tests). Stage 4 (multimodal): 26 tests across `test_stream_vision_ingest.py` (3), `test_multimodal_turn_pipeline.py` (3), `test_memory_quality_policy.py` (5), `test_confirmation_idempotency.py` (8), and `test_stage3_compat.py` (7). Stage 5 (action pipeline): 78 tests across 8 files covering action execution, desktop adapters, circuit breakers, dead letter queue, and recovery scenarios. Stage 6 (reliability): 27 tests in `test_stage6_reliability.py` covering retry taxonomy, SLO budgets, and dependency locking. Stage 7 (observability): 25 tests across `test_stage7_chaos_recovery.py` (15 chaos scenarios) and `test_stage7_backup_restore.py` (10 backup/restore scenarios). v2.6-2.7 (companion): 17 cross-track tests in `test_v26_cross_track.py` plus 65 hardening tests and 92 runtime integration tests. v2.8 (deterministic ops): 104 milestone tests plus 52 hardening tests. v2.9 (system closure): 68 new tests (21 routing, 19 supervision, 28 memory hybrid) plus 24 post-close drill tests.
### 14.5 Compatibility Test Pattern
Stage-level compatibility tests ensure that new features do not break existing functionality. `test_stage2_compat.py` (7 tests) verifies the turn pipeline still works identically after Stage 3 changes. `test_stage3_compat.py` (7 tests) verifies sessions and streaming after Stage 4. `test_stage4_compat.py` (7 tests) verifies multimodal after Stage 5. This pattern creates a regression safety net where each stage's functionality is permanently verified by a dedicated compatibility test file.
### 14.6 The sys.path Pattern
A critical implementation detail: every integration test file that imports Gateway modules must include `sys.path.insert(0, r"S:\services\api-gateway")` at the top of the file. This is necessary because the Gateway modules are not installed as packages -- they exist as loose Python files in the service directory. The same pattern applies for tests importing from other services. This boilerplate is a known technical debt item; a shared `conftest.py` with automatic path configuration would eliminate the duplication.
### 14.7 Smoke Tests
Quick validation scripts verify basic service health without running the full test suite. `S:\scripts\smoke_turn.ps1` sends a single turn through the pipeline and verifies the response. `S:\scripts\smoke_stage3_voice.ps1` creates a WebSocket session, sends text, and verifies streaming response. `S:\scripts\smoke_stage4_multimodal.ps1` runs 16 checks including vision frame handling and Stage 2/3 regression verification. Smoke tests are designed to run in under 30 seconds and are the first verification step after any stack restart.
### 14.8 Soak Tests
Long-running stress tests verify sustained performance under load. `S:\scripts\soak_stage3_sessions.ps1` creates multiple concurrent sessions with configurable `-Sessions` and `-TurnsPerSession` parameters, measuring session creation latency, turn response time, and session cleanup. `S:\scripts\soak_stage4_multimodal.ps1` exercises the multimodal pipeline with configurable load, reporting p50/p95 latency summaries. `S:\scripts\soak_stage5_actions.ps1` executes 200+ actions across all 13 capabilities, verifying circuit breaker stability and SLO compliance. `S:\scripts\soak_stage6_latency.ps1` executes 240 actions with per-capability p50/p95/p99 latency reporting and SLO violation detection. `S:\scripts\soak_v28_rc1.ps1` runs 700 operations across all four v2.8 milestones (model cancellation, memory budgets, perception gates, operator sessions).
### 14.9 Chaos Tests
The `test_stage7_chaos_recovery.py` (15 tests) validates recovery from injected failure scenarios: adapter timeout injection (simulating a slow desktop executor), circuit breaker trip and automatic recovery (verifying the CLOSED->OPEN->HALF_OPEN->CLOSED lifecycle under real load), DLQ replay under concurrent load (ensuring dead letters can be replayed while new actions are executing), correlation ID survival through failures (verifying that correlation context is preserved even when downstream calls fail), RTO verification (confirming recovery time objective of <60s, with actual measured RTO consistently <1s), and service restart recovery (verifying the stack resumes normal operation after a controlled service restart).
### 14.10 Known Flaky Tests
Three flaky test patterns are documented in `S:\issues\`: `INFRA-FLAKY-CHAOS-TIMING.md` (chaos tests with timing sensitivity under heavy system load), `INFRA-FLAKY-OLLAMA-TIMEOUT.md` (Ollama cold-start timeouts when the model is not cached in GPU memory), and `INFRA-FLAKY-WS-RACE.md` (WebSocket race conditions during rapid connect/disconnect sequences). These tests are marked with the `infra_flaky` pytest marker and can be excluded from promotion gate runs with `pytest -m "not infra_flaky"`.
### 14.11 Running the Full Suite
To run all tests: `cd S:\ && S:\envs\sonia-core\python.exe -m pytest tests/ services/ -v --tb=short`. To run a specific stage: `pytest tests/integration/test_turn_pipeline.py -v`. To exclude flaky tests: `pytest -m "not infra_flaky" -v`. To run with timing output: `pytest --durations=20 -v`. The full suite takes approximately 3-5 minutes with all services running, depending on GPU warm-up time for Ollama.
---
## Chapter 15: Training Pipeline -- Fine-Tuning the VLM
### 15.1 Overview
SONIA includes a complete training pipeline for fine-tuning a Vision-Language Model (Qwen3-VL-32B-Instruct) on domain-specific conversation data. The fine-tuned model (`sonia-vlm:32b`) is the primary local inference model, trained to understand SONIA's persona, capabilities, and behavioral contract. The pipeline covers dataset preparation, LoRA fine-tuning on cloud GPU infrastructure (RunPod), model merging, quantization to GGUF format, and publishing to HuggingFace Hub. All training code resides in `S:\training\`.
### 15.2 Training Infrastructure
Fine-tuning runs on RunPod cloud GPU instances, specifically configured for 2x NVIDIA H100 SXM GPUs (160GB combined VRAM, connected via NVLink for fast inter-GPU communication). The RunPod setup script (`S:\training\runpod\setup_and_train.sh`) automates instance preparation: installing CUDA drivers, Python dependencies, Unsloth (the training optimization library), and mounting the training data. The choice of H100s over smaller GPUs is driven by the model size (32B parameters) and the need for bf16 precision training without quantization -- bitsandbytes quantization has known compatibility issues with the Qwen3-VL architecture.
### 15.3 Dataset Preparation
Training data comes from two sources. The primary dataset (`VocaborSilentii/SoniaTraining`, approximately 8,900 records) contains SONIA-specific conversation data covering persona traits, capability demonstrations, behavioral boundaries, safety protocols, and multi-turn interaction patterns. The supplementary dataset (`VocaborSilentii/glm-4.7-2000x`, approximately 2,000 records) provides reasoning distillation data from a larger model, teaching SONIA deeper analytical capabilities. The `S:\training\runpod\combine_datasets_v2.py` script downloads both datasets from HuggingFace, preprocesses them (stripping `<think>` reasoning blocks from the GLM data, injecting SONIA's system prompt into all conversations, deduplicating entries, normalizing formatting), and produces three splits: `sonia_combined_train.jsonl` (7.8MB training split), `sonia_combined_val.jsonl` (526KB validation split), and `sonia_combined_test.jsonl` (579KB test split). The data is formatted as multi-turn conversations in JSONL format, compatible with the chat template expected by Qwen3-VL.
### 15.4 LoRA Fine-Tuning
The training script at `S:\training\runpod\train_sonia_qwen3vl.py` implements LoRA (Low-Rank Adaptation) fine-tuning using the Unsloth library for optimization. LoRA configuration: rank=32, alpha=64, dropout=0.05, targeting the q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, and down_proj matrices. This targets all major weight matrices in the transformer architecture, providing comprehensive fine-tuning while training only ~1% of total parameters. Training hyperparameters: bf16 precision (no quantization), 2 epochs over the combined dataset, learning rate 1.5e-4 with cosine decay schedule, effective batch size 32 (achieved via gradient accumulation), maximum sequence length 4096 tokens, gradient checkpointing enabled (reduces VRAM usage at the cost of ~20% slower training). The training uses assistant-only loss masking, meaning the model only learns to predict assistant responses, not user messages or system prompts. This is critical for multi-turn conversations where the model should learn response patterns, not input patterns. Training takes approximately 4-6 hours on the 2x H100 configuration.
### 15.5 Model Merging and Export
After training, the LoRA adapter weights must be merged into the base model to produce a single deployable model. Two merge scripts are provided: `S:\training\runpod\merge_push_sharded.py` performs the standard merge, splitting the output into sharded SafeTensors files (necessary for models that exceed single-file size limits), and pushes the result to HuggingFace Hub. `S:\training\runpod\merge_streaming.py` provides a memory-efficient streaming merge for environments with limited RAM, processing one layer at a time. After merging, the model is published to HuggingFace via `S:\training\runpod\upload_to_hf.py`.
### 15.6 GGUF Quantization for Local Deployment
For local deployment via Ollama, the merged model must be quantized to GGUF format. `S:\training\runpod\quantize_gguf.py` and `quantize_gguf_v2.py` handle the conversion using llama.cpp's conversion tools (`S:\tools\llama.cpp\convert_hf_to_gguf.py`, 564KB). The quantization process: (1) Load the merged HuggingFace model. (2) Convert to GGUF format with configurable quantization level (Q4_K_M recommended for a balance of quality and speed, Q8_0 for higher quality at the cost of VRAM). (3) Output a single `.gguf` file. (4) Create an Ollama Modelfile that references the GGUF file. (5) Register with Ollama via `ollama create sonia-vlm:32b -f Modelfile`. The GGUF file can also be published to HuggingFace for distribution.
### 15.7 HuggingFace Release Pipeline
The `S:\training\hf-release\` directory provides a complete release pipeline for publishing trained models. `build_manifest.py` creates a release manifest with checksums for all model files. `verify_artifacts.py` validates that all expected files exist and checksums match. `smoke_load.py` performs a quick inference test to verify the model loads and generates coherent output. `upload_release.py` uploads the model to HuggingFace Hub with proper model card and metadata. `tag_release.py` creates a versioned release tag on the HuggingFace repository. `verify_hub.py` downloads the published model and verifies checksums match the local version. `run_release_gate.py` orchestrates the full release checklist: build manifest, verify artifacts, smoke test, upload, tag, and verify.
### 15.8 Persona Pipeline
The `S:\pipeline\` directory implements a persona management system for defining and evaluating SONIA's personality. `cli.py` provides a CLI with 5 subcommands: `build` (construct a persona manifest from identity anchors), `validate` (verify manifest against schema v1.1.0), `eval` (run the evaluation harness), `compare` (compare two persona versions), and `export` (export persona for deployment). The `text/identity_invariants.py` defines 13 identity anchors (core personality traits) across 3 severity levels: critical (must never be violated, such as "always prioritize safety"), important (should be maintained, such as "maintain professional but warm tone"), and suggested (nice-to-have, such as "use technical terminology when appropriate"). The `eval/harness.py` implements a 5-dimensional evaluation measuring persona consistency across: tone (conversational style consistency), knowledge (domain expertise accuracy), boundaries (safety boundary compliance), style (response formatting patterns), and values (alignment with stated principles). Schema v1.1.0 includes deterministic build IDs for reproducible persona artifacts.
---
## Chapter 16: Avatar System -- 3D Embodiment
### 16.1 Overview
SONIA's avatar system provides visual embodiment through a high-quality 3D female character model with animation, rendering, and web viewing capabilities. The system spans 3D modeling (Blender), game engine integration (Unity), web rendering (Three.js), and a live avatar model for real-time facial animation. The avatar system is currently at prototype maturity level -- high-quality assets exist but live integration with the voice pipeline is not yet complete.
### 16.2 3D Model Assets
The primary 3D model is the Female Advanced V2 3D Model, stored in `S:\ui\Female Advanced V2 3D Model\` with a copy in `S:\assets\avatar\`. The model includes: FBX mesh files (11MB total, compatible with Blender, Unity, and 3ds Max), 70+ high-resolution texture maps at 8K-16K resolution covering diffuse color, normal maps, specular maps, subsurface scattering (for realistic skin rendering), ambient occlusion, displacement maps, and material masks for per-region material assignment. Supporting assets include 26 HDR environment maps for image-based lighting (providing realistic reflections and ambient illumination), 5 IES light profiles for photographic-quality directional lighting, ACES 1.0.3 color management configuration for cinema-grade color accuracy, and 43 preview renders demonstrating various poses, lighting setups, and material configurations. A CLO3D garment project (`ballete/`) provides physically-simulated clothing.
### 16.3 Blender Pipeline
13 Blender Python scripts in `S:\scripts\` automate 3D operations. `blender_render_femadv.py` (38KB) implements a complete rendering pipeline: scene setup, lighting configuration (using HDR environments and IES profiles), material assignment, camera positioning, and batch rendering with configurable resolution and sample count. `blender_animate_femadv.py` (26KB) provides an animation system for creating facial expressions, gestures, and body poses. `blender_customize_sonia.py` (26KB) handles material customization including skin tone, eye color, hair color, and clothing materials. Additional diagnostic scripts include `blender_diagnose_materials.py` (material property inspection), `blender_dump_bones.py` (skeleton hierarchy export), `blender_dump_vertex_groups.py` (mesh weight analysis), `blender_list_actions.py` (animation action catalog), `blender_list_scene.py` (scene object inventory), and `blender_prep_scene.py` (automated scene preparation). A customized Blender file (`FemAdv_Sonia.blend`) contains the SONIA-specific version of the character with tailored materials, rigging adjustments, and saved render configurations. Batch rendering workflows are automated via `run_animate.bat`, `run_animate_and_save.bat`, and `convert_renders_to_mp4.ps1`.
### 16.4 Unity Integration
The `S:\assets\avatar\My project\` directory contains a Unity project with AI-driven animation capabilities. `AIAnimationController.cs` implements an animation controller that responds to SONIA's conversational state (speaking, listening, thinking, idle) with appropriate character animations. `AIProceduralAnimator.cs` generates procedural motion for subtle idle animations (breathing, blinking, micro-movements) that prevent the character from appearing static. `RealtimeMotionGenerator.cs` handles real-time lip synchronization and gestural animation driven by TTS output and conversation context. `VideoRecorder.cs` provides video capture for generating avatar content. ML Agents training configuration enables learned animation behaviors that can adapt to conversation patterns over time. The Unity project is configured for the Universal Render Pipeline (URP) for mobile-compatible rendering.
### 16.5 Live Avatar Model
The `S:\assets\avatar\Live-Avatar\` directory contains a HuggingFace LiveAvatar model (1.35GB SafeTensors format) for real-time facial animation driven by audio input. This model generates facial expressions and lip movements synchronized to SONIA's TTS output. The model takes audio waveforms as input and produces per-frame facial blend shape coefficients that drive the 3D character's face. When fully integrated, this would enable SONIA to have a visible face that moves naturally during conversation.
### 16.6 Web Viewer
The `S:\ui\sonia-avatar\` directory contains a React/Three.js web viewer. Components include: ChatPanel (text chat interface alongside the 3D view), ControlBar (playback and view controls), DiagnosticsPanel (real-time diagnostics overlay), ErrorBoundary (graceful error handling), and StatusIndicator (connection status display). The Three.js AvatarScene component handles 3D model loading, rendering, and camera management. The viewer uses a Vite build system with TypeScript. A simpler standalone viewer at `S:\ui\Female Advanced V2 3D Model\viewer.html` provides basic Three.js rendering without the React framework, useful for quick model inspection and development.
### 16.7 Integration Status
The avatar system components exist independently but are not yet integrated into the live runtime. The planned integration path: (1) Pipecat's TTS output feeds into the LiveAvatar model for real-time lip sync. (2) The avatar model's blend shape output drives the Three.js web viewer. (3) The web viewer connects to the Gateway via WebSocket for conversation state. (4) The Unity path provides a higher-fidelity alternative for desktop deployment. Current blockers: the WebSocket protocol for streaming avatar state is not yet defined, the LiveAvatar model inference latency has not been benchmarked against the <200ms voice pipeline target, and the Three.js viewer lacks the blend shape animation system needed to consume the model's output.
---
## Chapter 17: Release Engineering and Promotion Gates
### 17.1 Version Strategy
SONIA uses semantic versioning with stage-based development milestones. The version history spans 12 releases across 6 days of development: RC1 (Feb 8, initial baseline after repair), v2.4.0-stage4 (multimodal sessions), v2.5.0-stage5 (action pipeline and desktop adapters), v2.5.0-rc1 (reliability hardening), v2.5.0 GA (observability and recovery drills), v2.6.0 (companion experience layer), v2.7.0 (companion runtime integration), v2.8.0-rc1 (deterministic operations with 104 tests), v2.8.0 GA (52 hardening tests added), v2.9.0 (system closure with provider parity), v2.9.1 (legacy test isolation), v2.9.2 (legacy closure and schema freeze), and v2.10.0-dev (current HEAD, real VLM inference and MCP server).
### 17.2 Promotion Gate Evolution
The promotion gate concept evolved significantly across releases, growing from a 6-gate checklist to a comprehensive 16-gate automated verification system. The initial `promotion-gate.ps1` (v2.5.0) verified 6 gates: regression tests pass, health checks green, circuit breakers in CLOSED state, DLQ empty, dependencies locked, and release manifest valid. By v2.5.0 enhanced (`promotion-gate-v2.ps1`), the gate expanded to 12 checks adding: chaos test results, backup integrity verification, diagnostics snapshot capture, correlation ID end-to-end verification, rollback script validation, and incident bundle generation. The v2.6 gate (`promotion-gate-v26.ps1`) reached 16 gates with per-gate timing, JSON machine-readable reports, and schema validation. Each gate produces a pass/fail/skip result with timing information, and the aggregate report is written to `gate-report.json` in the release bundle. The latest gate script `S:\scripts\promotion-gate-v29.ps1` runs 12 gates with JSON output and per-gate timing.
### 17.3 Release Bundle Structure
Each release is archived in `S:\releases\v{X.Y.Z}\` containing: `release-manifest.json` (SHA-256 checksums for all source files, configurations, and dependencies -- the integrity anchor), `gate-report.json` (machine-readable promotion gate results with per-gate timing), `dependency-lock.json` (SHA-256 hashes for all pip packages), `requirements-frozen.txt` (exact package version pins), `CHANGELOG.md` or `CHANGELOG.txt` (version-specific change descriptions), `soak-report.json` (soak test results including p50/p95/p99 latency measurements when applicable), and `env/` directory containing environment snapshots (`conda-list.txt`, `pip-freeze.txt`). The release manifest uses SHA-256 to hash every file in the release, enabling post-deployment integrity verification: if any file has been modified after release, the manifest check fails.
### 17.4 Rollback Scripts
Two rollback scripts provide emergency version regression capability. `S:\scripts\rollback-to-stage5.ps1` rolls back to the v2.5.0-stage5 tag, which represents the last known-good state before the companion experience layer was added. `S:\scripts\rollback-to-v25.ps1` rolls back to the v2.5.0 GA tag with additional features: `-DryRun` support (shows what would change without executing), health check markers (verifies all services are healthy before and after rollback), and configuration snapshot preservation. Both scripts use `git checkout` to the target tag and restart all services.
### 17.5 Stamp Release Script
The `S:\scripts\stamp-release-v29.ps1` script generates a release manifest by: (1) Computing SHA-256 hashes for all Python source files in `S:\services\`. (2) Computing hashes for all configuration files in `S:\config\`. (3) Computing hashes for `requirements-frozen.txt` and `dependency-lock.json`. (4) Recording the git commit hash, tag, and branch. (5) Recording the Python version, key package versions, and GPU configuration. (6) Writing the manifest to `S:\releases\v{version}\release-manifest.json`. The stamp script is the final step in the release process, creating the integrity anchor that all future verification checks reference.
### 17.6 CI/CD Integration
A GitHub Actions workflow exists at `S:\.github\workflows\sonia-build-gate.yml` but is not actively used in the current development workflow. The workflow defines: import validation (verifying all Python files can be imported without errors), type checking (via pyright or mypy), integration test execution (against a running stack), and promotion gate execution. The CI/CD system is designed to run on push to protected branches and produce the same gate-report.json that the manual promotion gate produces. Activation of this CI/CD pipeline is a v2.11 priority.
---
## Chapter 18: Development Chronology -- The Six-Day Build
### 18.1 Overview
SONIA's development history is remarkable for its velocity. The entire system was built in approximately six days (February 8-14, 2026), accumulating 80 git commits across 10 version tags and producing 580+ integration tests. The development followed a strict stage-based model where each stage built on verified foundations -- no stage was started until the previous stage's promotion gate passed.
### 18.2 Day 1 -- February 8: Foundation Sprint
The project began with commit `df0e107` (RC1 baseline) establishing the core microservices architecture after a repair operation that resolved pre-existing infrastructure issues. Within the same day, six major feature branches were developed and merged. The voice pipeline infrastructure (Pipecat) arrived in 5 commits implementing a deterministic turn state machine, cancel-aware interrupts and TTS clients, a stage watchdog and guarded ASR client, per-turn latency tracking with JSONL telemetry, and 32 unit tests. The action safety layer came in 4 commits adding a policy engine with allow/confirm/deny classification, confirmation token manager with TTL and replay defense, an action guard pre-execution wrapper, and OpenClaw/EVA-OS integration with enriched healthz. Model Router profiles were built in 6 commits: routing profile schema with 6 deterministic profiles, deterministic routing engine with health/budget callbacks, health registry with quarantine and probe-based recovery, budget guard enforcing context/latency ceilings, JSONL route audit logger, and wiring into main.py with enriched healthz. Stage 4 (multimodal release), Stage 5 (action pipeline with 4 milestones), and Stage 6 (reliability hardening) were all tagged by end of day.
### 18.3 Day 2 -- February 9: Observability and Companion
Stage 7 was completed in 5 milestone commits: end-to-end correlation ID traceability (fixing 5 propagation gaps), incident bundle export with diagnostics snapshot endpoint, chaos and recovery certification suite (15 tests), backup/restore discipline with SHA-256 integrity verification, and release automation v2 (12-gate promotion checklist). The v2.5.0 GA release was tagged with full ops drill and release decision record. The companion experience layer (v2.6) followed with 9 commits implementing: 3-track scaffold (persona, vision, embodiment), Track A persona CLI and deterministic artifacts, Track A invariant severity levels with enforcement, Track B vision capture privacy hardening with zero-frame guarantees, Track B perception event contract with SceneAnalysis validation, Track C UI control ACK model with diagnostics panel, unified event envelope with correlation IDs (17 integration tests), comprehensive docs and changelog, and the 16-gate promotion checklist.
### 18.4 Day 3 -- February 10: Deterministic Operations
The companion runtime integration (v2.7) was completed with 5 milestones and 92 tests, followed by v2.7.0 GA with contract freeze. Then v2.8.0 development began with 4 milestones: model routing cancellation (allowing operators to abort long-running inference), memory integration (token budget enforcement for context retrieval), perception gate (bypass-proof vision-to-action safety), and operator UX (state machine for operator session lifecycle). This produced 104 milestone tests. The GA promotion added 52 hardening tests (totaling 565 integration tests) with cleanroom verification and a comprehensive soak test running 700 operations.
### 18.5 Day 4 -- February 11: System Closure
v2.9.0 (System Closure) represented the most architecturally significant single-day effort. Three major subsystems were completed: the Model Router received fully implemented Anthropic and OpenRouter providers (using raw httpx, no vendor SDKs), with routing policy support and proper error handling. EVA-OS received real ServiceSupervisor implementation with live `/healthz` probes, 5-state machine per service, event emission, and dependency graph tracking. The Memory Engine received HybridSearchLayer (BM25 + LIKE fallback), ProvenanceTracker (audit log), and token budget enforcement. Additionally, a hygiene sweep migrated all services to the FastAPI lifespan pattern, created `shared/version.py`, deduplicated requirements, and cleaned up gitignore. The day produced 68 new tests (21 routing + 19 supervision + 28 memory) plus 24 post-close drill tests.
### 18.6 Day 5 -- February 12-13: Legacy Closure
v2.9.1 focused on legacy test isolation: shimming deprecated imports so that tests from earlier stages continued to pass even as internal module structure evolved. Release notes were added. v2.9.2 completed the legacy closure sprint with three workstreams: legacy import closure (resolving all remaining deprecated import paths), flaky test stabilization (adding timing margins and retry logic to timing-sensitive tests), and environment fix (resolving conda path issues). The schema was frozen, meaning no further changes to the API contracts would be made in the 2.x series. New services, training pipeline components, datasets, and UI components were also added in this phase.
### 18.7 Day 6 -- February 13-14: v2.10 Development
The final day brought two significant commits. The feature commit (`c3fc18b`) introduced real VLM inference (connecting the Perception service to actual Qwen3-VL model inference through the Model Router rather than using stubs), the sentence-level chunker for the Memory Engine (replacing the fixed-size character chunker), MCP server boot (the new Claude Code integration bridge), and policy tests for the action pipeline. The hardening commit (`bd8b764`) swept across the entire codebase: memory-engine optimizations, model-router improvements, script fixes, and test updates. This brought the codebase to its current state on the `v2.10-dev` branch.
### 18.8 Development Velocity Analysis
The six-day development velocity was achieved through several factors: clear architectural vision defined on Day 1 (all three foundational documents written before any code), automated promotion gates preventing regressions (each stage was verified before the next began), disciplined stage-based development (no forward progress without backward compatibility), aggressive parallelism (feature branches for independent subsystems), and the single-developer model eliminating coordination overhead. The risk of this velocity is key-person dependency -- the codebase needs documentation density sufficient for onboarding additional developers, which is the purpose of this guide.
---
## Chapter 19: Running the Complete Stack
### 19.1 Pre-Flight Verification
Before starting the stack, verify all prerequisites: (1) Python environment exists: `S:\envs\sonia-core\python.exe --version` returns 3.11.x. (2) Ollama is running: `curl http://127.0.0.1:11434/api/tags` returns model list. (3) At least one model is available in Ollama. (4) All required ports (7000-7070) are free: `netstat -ano | findstr "7000 7010 7020 7030 7040 7050"` shows no listeners. (5) Required directories exist: `S:\data\memory\`, `S:\logs\services\`, `S:\state\pids\`. (6) Configuration is valid: `python -c "import json; json.load(open(r'S:\config\sonia-config.json'))"` succeeds. (7) GPU is available: `python -c "import torch; print(torch.cuda.is_available())"` returns True.
### 19.2 Starting the Stack
Option A (automated): Run `S:\start-sonia-stack.ps1` from an elevated PowerShell terminal. The script handles dependency ordering, health verification, and error reporting. Option B (manual, useful for debugging): Start services one at a time in dependency order. First, start Memory Engine: `cd S:\services\memory-engine && S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port 7020 --log-level info`. In a separate terminal, start Model Router: `cd S:\services\model-router && S:\envs\sonia-core\python.exe -m uvicorn main:app --host 127.0.0.1 --port 7010 --log-level info`. Wait for both to show "Uvicorn running" then verify: `curl http://127.0.0.1:7020/healthz` and `curl http://127.0.0.1:7010/healthz`. Continue with OpenClaw (port 7040), EVA-OS (port 7050), Pipecat (port 7030), and finally API Gateway (port 7000).
### 19.3 Health Verification
After all services are running, verify the complete stack health. Probe each service individually: `curl http://127.0.0.1:7000/healthz` (Gateway), `curl http://127.0.0.1:7010/healthz` (Router), `curl http://127.0.0.1:7020/healthz` (Memory), `curl http://127.0.0.1:7030/healthz` (Pipecat), `curl http://127.0.0.1:7040/healthz` (OpenClaw), `curl http://127.0.0.1:7050/healthz` (EVA-OS). Check EVA-OS aggregated status: `curl http://127.0.0.1:7050/status` should show all services as HEALTHY. Run the diagnostic script: `S:\scripts\diagnostics\doctor.ps1` for a comprehensive health report.
### 19.4 Your First Turn
Send a test turn through the full cognitive pipeline: `curl -X POST http://127.0.0.1:7000/v1/turn -H "Content-Type: application/json" -d "{\"user_id\": \"operator\", \"conversation_id\": \"first-turn\", \"input_text\": \"Hello Sonia, what can you do?\"}"`. The response should include: `reply` (SONIA's text response), `memories_recalled` (number of memories retrieved, likely 0 on first turn), `model_used` (which LLM provider generated the response), `latency` (end-to-end timing), and `correlation_id` (the req_xxx identifier for this turn). Verify the memory was stored: `curl -X POST http://127.0.0.1:7020/v1/search -H "Content-Type: application/json" -d "{\"query\": \"Hello Sonia\"}"` should return the stored turn.
### 19.5 Creating a Session
For multi-turn conversations with context continuity, create a session: `curl -X POST http://127.0.0.1:7000/v1/sessions -H "Content-Type: application/json" -d "{\"config\": {\"model\": \"sonia-vlm:32b\"}}"`. The response includes a `session_id`. Use this ID for subsequent turns: `curl -X POST http://127.0.0.1:7000/v1/turn -H "Content-Type: application/json" -d "{\"user_id\": \"operator\", \"conversation_id\": \"SESSION_ID_HERE\", \"input_text\": \"What did I just ask you?\"}"`. The second turn should recall the first turn's memory and provide a contextual response.
### 19.6 Testing Desktop Actions
Trigger a safe_read action (no approval needed): `curl -X POST http://127.0.0.1:7000/v1/actions -H "Content-Type: application/json" -d "{\"capability\": \"file.read\", \"arguments\": {\"path\": \"S:\\\\config\\\\sonia-config.json\"}}"`. For a guarded action that requires approval, trigger: `curl -X POST http://127.0.0.1:7000/v1/actions -H "Content-Type: application/json" -d "{\"capability\": \"clipboard.write\", \"arguments\": {\"text\": \"Hello from SONIA\"}}"`. The response will include a `confirmation_token`. Approve it: `curl -X POST http://127.0.0.1:7000/v1/actions/ACTION_ID/approve -H "Content-Type: application/json" -d "{\"token\": \"TOKEN_HERE\"}"`.
### 19.7 Running Diagnostics
Check circuit breaker state: `curl http://127.0.0.1:7000/v1/breakers/metrics` (should show all breakers in CLOSED state). Check dead letter queue: `curl http://127.0.0.1:7000/v1/dead-letters` (should be empty on a healthy stack). Take a system snapshot: `curl http://127.0.0.1:7000/v1/diagnostics/snapshot` (returns complete system state). Create a state backup: `curl -X POST http://127.0.0.1:7000/v1/backups` (creates timestamped backup in `S:\backups\state\`).
### 19.8 Shutting Down
Run `S:\stop-sonia-stack.ps1` to stop all services in reverse dependency order. Alternatively, stop services manually in reverse order: Gateway first, then Pipecat, EVA-OS, OpenClaw, Model Router, Memory Engine. Verify all processes stopped: `netstat -ano | findstr "7000 7010 7020 7030 7040 7050"` should show no listeners.
### 19.9 Common Troubleshooting
**Port already in use**: Run `S:\scripts\ops\kill-by-port.ps1 -Port XXXX` to kill the process using the port. **Ollama not responding**: Verify Ollama service is running (`ollama list`), restart if needed. **Python import errors**: Verify the correct conda environment is activated (`S:\envs\sonia-core\python.exe`). **Model not found**: Verify models are pulled in Ollama (`ollama list`). **Memory Engine database locked**: Check for stale `.wal` and `.shm` files in `S:\data\memory\`; delete them and restart. **WebSocket connection refused**: Verify the target service is running and the port is correct. **High latency**: Check GPU utilization (`nvidia-smi`); if VRAM is exhausted, consider reducing the number of loaded models.
---
## Chapter 20: What Remains -- Production Readiness Gap Analysis
### 20.1 Current State Assessment
SONIA is operationally complete for its target deployment scenario: a single operator on a single Windows machine with a high-end GPU. All 8 core services have real implementations (not stubs), 580+ integration tests pass, the safety model is fully enforced, memory persists across sessions, and the training pipeline has produced a working fine-tuned VLM. The architecture is sound, the code is well-tested, and the release engineering discipline is mature. However, significant gaps remain between the current single-operator prototype and the enterprise-ready platform envisioned in the roadmap.
### 20.2 Immediate Gaps (v2.10 GA)
Five items must be addressed to finalize v2.10: (1) **MCP Server hardening** -- the Claude Code bridge is new and needs production-grade error handling, reconnection logic, capability exposure limits, and integration tests. (2) **VLM inference robustness** -- real inference works but needs GPU memory management (graceful degradation when VRAM is exhausted), timeout handling for large images, and streaming inference support. (3) **Sentence chunker validation** -- the new chunker needs testing against edge cases: tables, code blocks, multi-language text, and very long documents. (4) **Configuration cleanup** -- fix the three stale path references identified in Chapter 12. (5) **Session file pruning** -- add automated cleanup of the 458 empty session files in `S:\data\sessions\`.
### 20.3 Short-Term Gaps (v2.11-2.12)
Seven items for near-term production readiness: (1) **Persistent vector index** -- save the HNSW index to disk and reload on startup, eliminating re-indexing delay on Memory Engine restarts. (2) **Session persistence** -- replace in-memory sessions with SQLite or Redis-backed storage, enabling session survival across Gateway restarts. (3) **User authentication** -- implement JWT or OAuth2 for multi-user support; the current system has zero authentication. (4) **TLS for inter-service communication** -- add self-signed certificates for localhost; any future networked deployment requires encrypted transport. (5) **Container packaging** -- create Docker/Podman images for reproducible deployment and resource limiting. (6) **CI/CD activation** -- complete the GitHub Actions workflow with import checks, type checking, and automated test execution. (7) **Voice production hardening** -- real-world voice testing for microphone quality handling, background noise robustness, and multi-accent ASR performance.
### 20.4 Medium-Term Gaps (v2.13-3.0)
Eight items for scalable deployment: (1) **PostgreSQL migration** -- replace SQLite with PostgreSQL for multi-user data isolation, concurrent access, and horizontal scaling. (2) **Message queue** -- introduce RabbitMQ or Redis Streams for asynchronous inter-service communication, replacing the current synchronous HTTP coupling. (3) **Kubernetes deployment** -- Helm charts and manifests for cloud-native deployment with auto-scaling and rolling updates. (4) **Multi-tenant isolation** -- per-user data partitioning in Memory Engine and session management. (5) **Plugin system** -- extensible tool/executor architecture for third-party OpenClaw capabilities (currently hardcoded to 13). (6) **Web/mobile client** -- a production React/Next.js companion application. (7) **Live avatar integration** -- connecting the avatar system to the real-time voice pipeline for synchronized lip sync and gestures. (8) **Prometheus/Grafana observability** -- proper metrics dashboards replacing the current JSONL-based monitoring.
### 20.5 Long-Term Vision (v3.0+)
Five items for enterprise readiness: (1) **Distributed deployment** -- running services across multiple machines with service discovery and load balancing. (2) **High availability** -- service replication, failover, and the 99.9% uptime target. (3) **Enterprise governance** -- SOC2/HIPAA compliance, RBAC/ABAC permission model, delegation workflows, compliance reporting. (4) **Advanced memory** -- knowledge graph construction, multi-modal memory (images, audio, documents), cross-session learning. (5) **OpenTelemetry** -- replacing correlation-ID-based tracing with full distributed tracing.
### 20.6 Technical Debt Summary
17 identified debt items ranked by severity. Critical: stale configuration references (3 conflicting path references), in-memory session state (lost on restart), non-persistent HNSW vector index (rebuilt on every restart). Structural: legacy tool-service directory (overlaps with OpenClaw), dual JSON/YAML configuration (conflicting values), monolithic main.py files (Gateway 33.5KB, Memory Engine 29KB), 458 empty session files, baselines directory (50MB of duplicate early snapshots). Operational: no container packaging, PowerShell-only operations (no cross-platform support), no active CI/CD, no authentication, no TLS. Code-level: `__pycache__` in git, test boilerplate duplication (sys.path.insert), completion reports mixed with source code, duplicate FBX model files.
### 20.7 The Path Forward
The recommended evolution strategy proceeds in three phases. Phase 1 (Current through v2.12): harden the single-machine deployment -- fix configuration debt, persist vectors, add session persistence, implement authentication, activate CI/CD. Goal: production-grade reliability for a single operator. Phase 2 (v2.13 through v2.15): add team deployment infrastructure -- PostgreSQL, Redis, Docker containers, basic multi-tenancy. Goal: supporting 2-5 concurrent operators. Phase 3 (v3.0+): full distributed deployment -- Kubernetes, service mesh, horizontal scaling, enterprise governance features. Goal: 100+ concurrent users with high availability guarantees. Each phase builds on verified foundations, following the same stage-based development discipline that produced the current system.
---
## Appendix A: Complete Port Map
| Port | Service | Status | Health Endpoint | Dependencies |
|------|---------|--------|-----------------|-------------|
| 7000 | API Gateway | Active | /healthz | Model Router, Memory Engine, OpenClaw |
| 7010 | Model Router | Active | /healthz | Ollama (11434) |
| 7020 | Memory Engine | Active | /healthz | None |
| 7030 | Pipecat | Active | /healthz | Model Router, API Gateway |
| 7040 | OpenClaw | Active | /healthz | None |
| 7050 | EVA-OS | Active | /healthz | All core services (monitoring) |
| 7060 | Vision Capture | Optional | /healthz | None |
| 7070 | Perception | Optional | /healthz | Vision Capture, Model Router |
| 8000 | Orchestrator | Separate | N/A | API Gateway |
| 11434 | Ollama | External | /api/tags | GPU/CUDA |
## Appendix B: Full Technology Stack
| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| Language | Python | 3.11 | All services |
| Web Framework | FastAPI | 0.116.1 | HTTP/WS routing |
| ASGI Server | Uvicorn | 0.35.0 | Service hosting |
| Validation | Pydantic | 2.11.7 | Request/response models |
| HTTP Client | httpx | 0.28.1 | Inter-service calls |
| WebSocket | websockets | 16.0 | Real-time streaming |
| Database | SQLite 3 | WAL mode | Memory persistence |
| ML Framework | PyTorch | 2.10.0+cu128 | Model inference/training |
| ML Training | Unsloth | 2026.1.4 | LoRA fine-tuning |
| Training Tools | TRL, PEFT, accelerate | 0.24.0, 0.18.1, 1.12.0 | Training pipeline |
| Quantization | bitsandbytes | 0.49.1 | Model compression |
| Tokenizers | tokenizers, sentencepiece | 0.22.2, 0.2.1 | Text tokenization |
| ASR Model | faster-whisper-large-v3 | CTranslate2 | Speech recognition |
| VLM Model | Qwen3-VL-32B-Instruct | Custom fine-tune | Vision-language inference |
| Embedding Model | Qwen3-Embedding-8B | GGUF f16 | Vector embeddings |
| Reranker Model | Qwen3-Reranker-8B | GGUF f16 | Search reranking |
| TTS Model | Qwen3-TTS-Tokenizer | 12Hz | Speech synthesis |
| LLM Inference | Ollama | Latest | Local model serving |
| Cloud LLM | Anthropic API | httpx | Claude models |
| Cloud LLM | OpenRouter API | httpx | Multi-model access |
| 3D Rendering | Blender | 3.6+ | Avatar rendering |
| Game Engine | Unity | 2022+ | Avatar animation |
| Web 3D | Three.js | Latest | Avatar web viewer |
| UI Framework | React | TypeScript | Companion UI |
| Build Tool | Vite | Latest | Web UI bundling |
| MCP Protocol | FastMCP | Latest | Claude Code bridge |
| Operating System | Windows 11 | Pro/Enterprise | Runtime target |
| Shell | PowerShell | 5.1 | Operations tooling |
| Version Control | Git | Latest | Source management |
| Package Manager | pip (frozen), conda (env) | Latest | Dependency management |
## Appendix C: Configuration File Index
| File | Format | Purpose | Location |
|------|--------|---------|----------|
| sonia-config.json | JSON | Canonical configuration (all services) | S:\config\ |
| model-routing.yaml | YAML | Model routing rules and fallback chains | S:\config\models\ |
| services.yaml | YAML | Service definitions (supplementary) | S:\config\services\ |
| voice-profile.yaml | YAML | Voice synthesis profiles | S:\config\voice\ |
| default.yaml | YAML | Default policy configuration | S:\config\policies\ |
| app.yaml | YAML | Application-level settings | S:\config\ |
| ports.yaml | YAML | Port-to-service mapping | S:\configs\ |
| logging.yaml | YAML | Log formatters and handlers | S:\configs\ |
| default.yaml | YAML | Default configuration values | S:\configs\ |
| dependency-lock.json | JSON | SHA-256 hashes for all packages | S:\config\ |
| requirements-frozen.txt | Text | Exact package version pins (80 packages) | S:\ |
| pytest.ini | INI | Test configuration | S:\ |
| .gitignore | Text | Version control exclusions | S:\ |
| sonia-build-gate.yml | YAML | GitHub Actions CI/CD workflow | S:\.github\workflows\ |
## Appendix D: Test File Inventory
**Integration Tests (S:\tests\integration\):**
test_turn_pipeline.py (8 tests, Stage 2), test_session_lifecycle.py (5, Stage 3), test_stream_text_fallback.py (4, Stage 3), test_tool_confirmation_gate.py (9, Stage 3), test_stage2_compat.py (7, regression), test_stream_vision_ingest.py (3, Stage 4), test_multimodal_turn_pipeline.py (3, Stage 4), test_memory_quality_policy.py (5, Stage 4), test_confirmation_idempotency.py (8, Stage 4), test_stage3_compat.py (7, regression), test_stage6_reliability.py (27, Stage 6), test_stage7_chaos_recovery.py (15, Stage 7), test_stage7_backup_restore.py (10, Stage 7), test_v26_cross_track.py (17, v2.6), test_v27_runtime_integration.py (multiple, v2.7), test_v28_model_routing.py (multiple, v2.8), test_v28_memory_integration.py (multiple, v2.8), test_v28_perception_gate.py (multiple, v2.8), test_v28_operator_session.py (multiple, v2.8), test_v29_routing_drills.py (21, v2.9), test_v29_supervision_drills.py (19, v2.9), test_v29_memory_hybrid.py (28, v2.9).
**Model Router Tests (S:\tests\model_router\):**
test_budget_guard.py, test_health_quarantine.py, test_profile_selection.py, test_fallback_matrix.py (~40 tests total).
**Pipecat Tests (S:\tests\pipecat\):**
test_interrupt_handling.py, test_turn_state_machine.py, test_watchdog.py (~30 tests total).
**Safety Tests (S:\tests\safety\):**
test_policy_engine.py, test_confirmation_flow.py (~20 tests total).
**Service Contract Tests (S:\services\*\test_contract.py):**
5 files, ~75 tests verifying API contract compliance per service.
**Total: ~580+ tests across 57 files.**
---
**End of Guide**
*SONIA Final Build Guide v1.0 -- Generated February 14, 2026*
*Total: 20 chapters + 4 appendices*
*Covers: Architecture, environment, all 10 services, configuration, operations, testing, training, avatar, release engineering, development chronology, operational walkthrough, and production gap analysis*
